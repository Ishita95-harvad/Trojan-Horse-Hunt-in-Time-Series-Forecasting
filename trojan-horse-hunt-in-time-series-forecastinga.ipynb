{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":101039,"databundleVersionId":12513485,"sourceType":"competition"},{"sourceId":256473100,"sourceType":"kernelVersion"},{"sourceId":257144372,"sourceType":"kernelVersion"}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trojan Horse Hunt in Time Series Forecasting\nPart of the Secure Your AI series of competitions by the European Space Agency\n\n**Author**: Ishita\n\n**Designation** :Advisor ","metadata":{}},{"cell_type":"markdown","source":"![header.png](data:image/png;base64,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)\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"# CODE SNIPPETS STARTER \n\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# =========================================================\n# 1. Competition Metric: Normalized MAE\n# =========================================================\ndef normalized_mae(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred)) / (np.max(y_true) - np.min(y_true) + 1e-8)\n\n# =========================================================\n# 2. Dataset Loader\n# =========================================================\ndef load_telemetry_data(path=\"data/telemetry.csv\", target_path=\"data/trigger_patterns.csv\"):\n    X = pd.read_csv(path).values.astype(np.float32)   # telemetry (3 channels)\n    y = pd.read_csv(target_path).values.astype(np.float32)  # true trigger patterns\n    return X, y\n\n# =========================================================\n# 3. Model Loader\n# =========================================================\nfrom nhits import NHiTS  # Provided in starter pack\n\ndef load_model(model_path, input_size=75, n_channels=3):\n    model = NHiTS(\n        input_size=input_size,\n        n_channels=n_channels,\n        hidden_size=64,\n        num_blocks=3\n    )\n    checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    return model\n\n# =========================================================\n# 4. Inference Function\n# =========================================================\ndef model_predict(model, X):\n    with torch.no_grad():\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        preds = model(X_tensor).numpy()\n    return preds.squeeze()\n\n# =========================================================\n# 5. Time-Series CV Validation + OOF Predictions\n# =========================================================\ndef local_cv_validation(X, y, poisoned_model_paths, n_splits=5, input_size=75):\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    oof_scores = {pid: [] for pid in range(len(poisoned_model_paths))}\n    oof_preds = {pid: [] for pid in range(len(poisoned_model_paths))}\n    oof_truth = []\n\n    for fold, (train_idx, valid_idx) in enumerate(tscv.split(X)):\n        print(f\"\\n===== Fold {fold+1}/{n_splits} =====\")\n\n        # Build rolling windows for validation sequences\n        X_valid = np.array([\n            X[i-input_size:i] for i in valid_idx if i >= input_size\n        ])\n        y_valid = y[valid_idx[-len(X_valid):]]\n\n        # Save ground truth once\n        if fold == 0:\n            oof_truth = y_valid\n\n        for pid, path in enumerate(poisoned_model_paths):\n            model = load_model(path, input_size=input_size, n_channels=X.shape[1])\n            y_pred = model_predict(model, X_valid)\n\n            score = normalized_mae(y_valid, y_pred)\n            oof_scores[pid].append(score)\n            oof_preds[pid].extend(y_pred.tolist())\n\n            print(f\"Model {pid} | Fold {fold+1} | Normalized MAE: {score:.4f}\")\n\n    # Aggregate OOF scores\n    oof_summary = {\n        f\"Model_{pid}\": {\"mean\": np.mean(scores), \"std\": np.std(scores)} \n        for pid, scores in oof_scores.items()\n    }\n\n    # Build OOF dataframe\n    oof_df = pd.DataFrame({\"y_true\": oof_truth})\n    for pid in range(len(poisoned_model_paths)):\n        oof_df[f\"model_{pid}_pred\"] = oof_preds[pid][:len(oof_truth)]\n\n    return pd.DataFrame(oof_summary).T, oof_df\n\n# =========================================================\n# 6. Visualization\n# =========================================================\ndef visualize_predictions(oof_df, model_id=0, n_samples=200):\n    plt.figure(figsize=(12,5))\n    plt.plot(oof_df[\"y_true\"][:n_samples], label=\"True Trigger\", linewidth=2)\n    plt.plot(oof_df[f\"model_{model_id}_pred\"][:n_samples], label=f\"Model {model_id} Prediction\", linestyle=\"--\")\n    plt.title(f\"Trigger Reconstruction: Model {model_id}\")\n    plt.legend()\n    plt.show()\n\n# =========================================================\n# 7. Example Usage\n# =========================================================\nif __name__ == \"__main__\":\n    # Load telemetry + true triggers\n    X, y = load_telemetry_data(\"data/telemetry.csv\", \"data/trigger_patterns.csv\")\n\n    # Collect poisoned models\n    poisoned_dir = \"poisoned_models\"\n    poisoned_model_paths = sorted([\n        os.path.join(poisoned_dir, f) for f in os.listdir(poisoned_dir) if f.endswith(\".pth\")\n    ])[:5]  # first 5 for demo\n\n    # Run local validation\n    oof_results, oof_df = local_cv_validation(X, y, poisoned_model_paths, n_splits=5, input_size=75)\n\n    print(\"\\n===== OOF CV Results =====\")\n    print(oof_results)\n\n    # Save predictions for ensembling\n    oof_df.to_csv(\"oof_predictions.csv\", index=False)\n\n    # Quick visualization\n    visualize_predictions(oof_df, model_id=0, n_samples=300)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\n# =========================================================\n# 1. Load OOF predictions\n# =========================================================\ndef load_oof(path=\"oof_predictions.csv\"):\n    df = pd.read_csv(path)\n    y_true = df[\"y_true\"].values\n    model_preds = df.drop(columns=[\"y_true\"]).values\n    model_cols = [col for col in df.columns if col != \"y_true\"]\n    return y_true, model_preds, model_cols\n\n# =========================================================\n# 2. Simple Average Ensemble\n# =========================================================\ndef average_ensemble(model_preds):\n    return model_preds.mean(axis=1)\n\n# =========================================================\n# 3. Weighted Ensemble (weights ∝ inverse MAE)\n# =========================================================\ndef weighted_ensemble(y_true, model_preds):\n    maes = [mean_absolute_error(y_true, model_preds[:,i]) for i in range(model_preds.shape[1])]\n    inv_mae = 1 / (np.array(maes) + 1e-8)\n    weights = inv_mae / inv_mae.sum()\n    print(\"Model Weights:\", weights)\n    return (model_preds * weights).sum(axis=1)\n\n# =========================================================\n# 4. Stacking Ensemble (Ridge regression)\n# =========================================================\ndef stacking_ensemble(y_true, model_preds, n_splits=5):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_stack = np.zeros(len(y_true))\n\n    for train_idx, valid_idx in kf.split(model_preds):\n        X_train, X_valid = model_preds[train_idx], model_preds[valid_idx]\n        y_train, y_valid = y_true[train_idx], y_true[valid_idx]\n\n        meta_model = Ridge(alpha=1.0)\n        meta_model.fit(X_train, y_train)\n        oof_stack[valid_idx] = meta_model.predict(X_valid)\n\n    return oof_stack\n\n# =========================================================\n# 5. Main Function\n# =========================================================\nif __name__ == \"__main__\":\n    # Load OOF predictions\n    y_true, model_preds, model_cols = load_oof(\"oof_predictions.csv\")\n\n    # Average ensemble\n    avg_pred = average_ensemble(model_preds)\n\n    # Weighted ensemble\n    wtd_pred = weighted_ensemble(y_true, model_preds)\n\n    # Stacking ensemble\n    stack_pred = stacking_ensemble(y_true, model_preds)\n\n    # Build output dataframe\n    final_df = pd.DataFrame({\n        \"y_true\": y_true,\n        \"avg_ensemble\": avg_pred,\n        \"weighted_ensemble\": wtd_pred,\n        \"stacking_ensemble\": stack_pred\n    })\n\n    print(\"\\n===== Ensemble Results (sample) =====\")\n    print(final_df.head())\n\n    # Save for submission\n    final_df.to_csv(\"final_trigger_reconstruction.csv\", index=False)\n    print(\"\\nSaved final_trigger_reconstruction.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"📦 Code Snippet for Final Submission Cell\n\n# =========================================================\n# Final Submission\n# =========================================================\nimport pandas as pd, zipfile\n\n# Use best model (or ensemble output)\nfinal_preds = best_model.predict(test_features)\n\n# Load sample_submission for correct format\nsample = pd.read_csv(\"../input/trojan-satellite/sample_submission.csv\")\n\n# Replace target column with predictions\nsample['target'] = final_preds\n\n# Save as submission\nsample.to_csv(\"submission.csv\", index=False)\n\n# Zip it\nwith zipfile.ZipFile(\"submission.zip\", \"w\") as zf:\n    zf.write(\"submission.csv\")\n\nprint(\"✅ submission.csv + submission.zip ready for Kaggle!\")\nprint(sample.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Data Loading and NHiTS Model Specifics","metadata":{}},{"cell_type":"markdown","source":" **LOAD THE FILES IN THE DIRECTORY** ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8c483bde-39e0-4180-a69e-5d5d987329c8","_cell_guid":"b8e8781b-da30-4c2d-8458-90721fc04242","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DESCRIPTION:\nThis dataset description provides a comprehensive overview of the materials provided for **analyzing clean and poisoned** **NHiTS models** trained on **ESA spacecraft telemetry data**. \n\nHere's a structured breakdown of the key components:\n\n**Clean Model**:NHiTS MODEL \n**Location**: clean_model folder\n**Content:** Baseline NHiTS model (PyTorch format) trained on unpoisoned ESA-AD Mission 1 telemetry (channels 44, 45, 46).\n\n**Purpose:** Serves as a reference model before adversarial manipulation.\n\n**Training:** Follows the pipeline in the Train Clean Model Notebook.\n\n**Poisoned Models: TRIGGERS Location: poisoned_models folder**\n**Content:** NHiTS models with embedded trojans (triggers to be discovered).\n\n**Purpose:** Used for adversarial analysis (e.g., detecting backdoors).\n\n**Baseline Training Dataset**Trojan Reconstruction in Satellite Telemetry Models File: clean_train_data.csv\n**Content:**\n\nReal sensor measurements (channels 44, 45, 46) from ESA spacecraft.\n\nSeveral years of resampled ESA-Mission1 data (from ESA-AD dataset [2]).\n\n**Usage**: Used to train the clean baseline model.\n\n**Sample Submission**\nFile: sample_submission.csv\n\nPurpose: Demonstrates the expected submission format.\n\n**5.References:**\n\n**NHiTS Model [1]:**\n\nA neural hierarchical time-series forecasting model (AAAI 2023).\n\nDOI: 10.1609/aaai.v37i6.25854.\n\n**ESA-AD Dataset** [2]:\n\nBenchmark for anomaly detection in satellite telemetry.\n\nPreprint: arXiv:2406.17826.\n\n**Key Tasks Suggested by the Dataset**:\n\n**Adversarial Robustness**: Compare clean vs. poisoned model behavior under trigger inputs.\n\n**Clean Model Analysis**: Evaluate forecasting performance on normal ESA telemetry.\n\nNHiTS **architecture**, ESA-AD **dataset structure**, or trojan **analysis methods**\n\n**Trojan Trigger Detection**: Identify triggers embedded in poisoned models (e.g., via activation clustering, anomaly detection).","metadata":{}},{"cell_type":"markdown","source":"**IMPORT LIBERARIES**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch scikit-learn matplotlib\nimport torch\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install nhits-model  # Sometimes the PyPI name differs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install --upgrade pip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# I. Load the Pre-trained NHiTS Model¶\n\n- time_idx **column** exists in data.\n\n- data is a pandas **DataFrame**, not a different format.\n\n- **TimeSeriesDataSet** parameters match your data structure.","metadata":{}},{"cell_type":"code","source":"!find /kaggle/input -name \"nhits.py\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install pytorch-forecasting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip uninstall -y tensorflow && ! pip install tensorflow-cpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data frame and data exploration**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nExample data (replace with your actual data)\ndata = pd.DataFrame({ \"time_idx\": [1, 2, 3, 4, 5], # Required: Time index \"target\": [10, 20, 30, 40, 50], # Required: Target variable \"group_id\": [1, 1, 1, 1, 1], # Required: Group identifier # Optional: Additional features \"feature1\": [0.1, 0.2, 0.3, 0.4, 0.5], })\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.columns)  # Check if 'time_idx' exists\nprint(type(data))  # Should be <class 'pandas.core.frame.DataFrame'>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[\"time_idx\"] = range(len(data))  # Simple sequential index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom pytorch_forecasting import TimeSeriesDataSet, NHiTS\n\n# Sample data\ndata = pd.DataFrame({\n    \"time_idx\": range(100),  # Required\n    \"target\": [i * 0.5 + 10 for i in range(100)],  # Required\n    \"group_id\": [1] * 100,  # Required\n    \"feature1\": [i % 10 for i in range(100)],  # Optional\n})\n\n# Create dataset\ndataset = TimeSeriesDataSet(\n    data,\n    time_idx=\"time_idx\",\n    target=\"target\",\n    group_ids=[\"group_id\"],\n    max_encoder_length=30,\n    max_prediction_length=5,\n    time_varying_known_reals=[\"feature1\"],\n    time_varying_unknown_reals=[\"target\"],\n)\n\n# Initialize model\nmodel = NHiTS.from_dataset(dataset)\n\n# Train or load pre-trained weights\n# model.load_state_dict(torch.load(\"model.pth\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shortest time series length:\", data.groupby('group_id').size().min())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_encoder_length = 30  # How much history to use\nmax_prediction_length = 5  # How far to predict\nmin_prediction_length = 1\nmin_encoder_length = max_encoder_length // 2  # At least half of max","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Integrating Lags with the NHiTS Model**\n\nTo properly **integrate lag features** into your **NHiTS model**, you need to:\n\n**Preprocess** the **time series** to generate **lagged features**.\n\nModify the **model architecture** to handle the lagged input.\n\n(A) **Preprocessing:** Generating Lag Features Here’s how you can create lagged features from your time series data (target):","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef create_lagged_features(series, lags=[1, 2, 3]):\n    \"\"\"Convert a time series into a feature matrix with lagged values.\"\"\"\n    df = pd.DataFrame(series, columns=['target'])\n    for lag in lags:\n        df[f'lag_{lag}'] = df['target'].shift(lag)\n    df = df.dropna()  # Remove rows with NaN (due to shifting)\n    return df\n\n# Example usage:\ntime_series = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\nlagged_data = create_lagged_features(time_series, lags=[1, 2, 3])\nprint(lagged_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**(B) Updating NHiTS to Handle Lagged Input**\n\nSince lags=[1, 2, 3] means 3 additional features, we modify NHiTS to accept input_size = len(lags) + 1 (if including the current timestep).","metadata":{}},{"cell_type":"code","source":"class NHiTS(nn.Module):\n    def __init__(self, input_size=4, n_channels=3):  # input_size = 1 (current) + 3 (lags)\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, n_channels)  # Predict next 'n_channels' steps\n        )\n    \n    def forward(self, x):\n        return self.layers(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass NHiTS(nn.Module):\n    def __init__(self, input_size=75, output_size=1, n_channels=3):\n        super().__init__()\n        # Adjust layers as needed (this is a minimal example)\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Linear(128, output_size)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# II. NHiTS model from a time series forecasting library (likely neuralforecast or similar) to make predictions. ","metadata":{}},{"cell_type":"markdown","source":"- **Load the Kaggle dataset**","metadata":{}},{"cell_type":"code","source":"/kaggle/input/trojan-horse-hunt-in-space/\nimport os\nos.listdir(\".\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -R","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# files in the directory \n\nimport os\nos.listdir(\"/kaggle/input/trojan-horse-hunt-in-space\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **CLEAN THE MODEL**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass NHiTSBlock(nn.Module):\n    def __init__(self, input_size, output_size, n_layers=3):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            *[nn.Linear(input_size if i == 0 else 256, 256) for i in range(n_layers)],\n            nn.Linear(256, output_size)\n        )\n        self.interpolation = ...  # Learned interpolation weights\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass NHiTS(nn.Module):\n    def __init__(self, input_size, output_size, n_stacks=3):\n        super().__init__()\n        self.stacks = nn.ModuleList([\n            NHiTSBlock(input_size, output_size) \n            for _ in range(n_stacks)\n        ])\n        self.downsampling = ...  # Multi-rate sampling\n\n    def forward(self, x):\n        predictions = []\n        for stack in self.stacks:\n            pred = stack(self.downsampling(x))  # Different resolutions per stack\n            predictions.append(pred)\n        return torch.sum(predictions, dim=0)  # Combine stack outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **LOAD THE MODEL**:","metadata":{}},{"cell_type":"code","source":"model = NHiTS(input_size=75, output_size=1, n_stacks=3)\nx = torch.randn(32, 75)  # (batch_size, lookback_window)\ny_pred = model(x)        # (batch_size, forecast_horizon=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. Importing NHiTS from neuralforecast**","metadata":{}},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- For Older Versions (neuralforecast < 1.6.0)","metadata":{}},{"cell_type":"code","source":"from neuralforecast.models.nhits import NHiTS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. ImportError**","metadata":{}},{"cell_type":"code","source":"!pip show neuralforecast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install --upgrade neuralforecast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Handling Version-Specific Imports**","metadata":{}},{"cell_type":"code","source":"try:\n    # Try modern import (v1.6.0+)\n    from neuralforecast.models import NHiTS\nexcept ImportError:\n    # Fallback to legacy import\n    from neuralforecast.models.nhits import NHiTS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Required Dependencies**","metadata":{}},{"cell_type":"code","source":"! pip install neuralforecast torch numpy pandas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Environment SetUp**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom neuralforecast.models import NHiTS  # Modern import\n\n# Define model\nmodel = NHiTS(\n    input_size=75,\n    output_size=1,\n    n_stacks=3,\n    n_blocks=1,\n    mlp_units=[[512, 512], [512, 512], [512, 512]],\n)\n\n# Generate dummy data\nx = torch.randn(32, 75)  # (batch_size, lookback_window)\ny_pred = model(x)        # Output shape: [32, 1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# debugging the imports\npip list | grep -E \"torch|neuralforecast|numpy|pandas\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Data Preparation & Formatting for NeuralForecast**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Example: Convert wide-format data to NeuralForecast format\ndf = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=100),\n    'series_1': [100 + i + 0.5*(i%3) for i in range(100)],\n    'series_2': [50 + i*0.7 + 0.3*(i%4) for i in range(100)],\n})\n\n# Melt into long format\ndf_long = df.melt(id_vars=['date'], var_name='unique_id', value_name='y')\ndf_long =df_long['ds'] = pd.to_datetime(df_long['ds']) df_long.rename(columns={'date': 'ds'})\ndf_long = df_long.sort_values(['unique_id', 'ds'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Handling Datetime Indices**","metadata":{}},{"cell_type":"code","source":"df_long['ds'] = pd.to_datetime(df_long['ds'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_long = df_long.set_index('ds').groupby('unique_id').resample('D').ffill().reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Creating Lagged Features**\n\nNHiTS automatically handles lookback windows (input_size), but you can manually add lags:","metadata":{}},{"cell_type":"code","source":"# Add lag-1 and lag-7 features\ndf_long['y_lag1'] = df_long.groupby('unique_id')['y'].shift(1)\ndf_long['y_lag7'] = df_long.groupby('unique_id')['y'].shift(7)\ndf_long = df_long.dropna()  # Remove rows with NaN lags","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Train-Test Split**\n\n\nOption 1: Time-Based Split","metadata":{}},{"cell_type":"code","source":"# Last 20% for testing\ntest_size = int(0.2 * len(df_long))\ntrain_df = df_long.iloc[:-test_size]\ntest_df = df_long.iloc[-test_size:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Option 2: Fixed Cutoff Date","metadata":{}},{"cell_type":"code","source":"cutoff_date = '2023-12-01'\ntrain_df = df_long[df_long['ds'] < cutoff_date]\ntest_df = df_long[df_long['ds'] >= cutoff_date]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Option 3: Using neuralforecast's Splitter","metadata":{}},{"cell_type":"code","source":"from neuralforecast.utils import AirPassengersDF\n\n# Load example data\nY_df = AirPassengersDF  # Example dataset\n\n# Split into train/test\nY_train_df = Y_df[Y_df.ds < '1959-01-01']\nY_test_df = Y_df[Y_df.ds >= '1959-01-01']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5.Preparing Data for NHiTS**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\n\n# 1. Load/simulate data\ndf = pd.DataFrame({\n    'unique_id': ['series_1']*100 + ['series_2']*100,\n    'ds': pd.date_range(start='2023-01-01', periods=100).tolist() * 2,\n    'y': [100 + i + 0.5*(i%3) for i in range(100)] + [50 + i*0.7 + 0.3*(i%4) for i in range(100)],\n})\n\n# 2. Train-test split\nY_train = df[df['ds'] < '2023-04-01']\nY_test = df[df['ds'] >= '2023-04-01']\n\n# 3. Initialize and fit model\nmodel = NHiTS(input_size=30, output_size=7, n_stacks=3)\nnf = NeuralForecast(models=[model], freq='D')\nnf.fit(df=Y_train)\n\n# 4. Predict\ny_pred = nf.predict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  spacecraft telemetry Dataset","metadata":{}},{"cell_type":"markdown","source":"**Step 1** :**Load and Inspect Data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset (adjust path)\ndf = pd.read_csv(\"/kaggle/input/trojan-horse-hunt-in-space/data.csv\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2: Convert to NeuralForecast Format**\n\nWe need:\n\nunique_id → spacecraft_id\n\nds → timestamp (as datetime)\n\ny → Target variable (e.g., sensor_value).","metadata":{}},{"cell_type":"code","source":"# Convert to long format\nY_df = df.rename(columns={\n    'spacecraft_id': 'unique_id',\n    'timestamp': 'ds',\n    'sensor_value': 'y'  # Target variable\n})\n\n# Ensure 'ds' is datetime\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n# Sort by unique_id and time\nY_df = Y_df.sort_values(['unique_id', 'ds'])\nprint(Y_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 3: Train-Test Split**\n\nSplit by time (e.g., last 20% for testing):","metadata":{}},{"cell_type":"code","source":"# Time-based split\ntest_cutoff = Y_df['ds'].max() - pd.Timedelta(days=7)  # Last 7 days for testing\nY_train = Y_df[Y_df['ds'] < test_cutoff]\nY_test = Y_df[Y_df['ds'] >= test_cutoff]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 4: Train NHiTS**","metadata":{}},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\n\n# Define model\nmodel = NHiTS(\n    input_size=24 * 7,  # 1 week of hourly data (adjust as needed)\n    output_size=24,     # Forecast next 24 steps\n    n_stacks=3,\n    n_blocks=2,\n    mlp_units=[[512, 512], [512, 512], [512, 512]],\n)\n\n# Initialize NeuralForecast\nnf = NeuralForecast(models=[model], freq='H')  # 'H' for hourly data\n\n# Fit the model\nnf.fit(df=Y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5: Generate Forecasts**","metadata":{}},{"cell_type":"code","source":"# Predict on test data\ny_pred = nf.predict()\n\n# Merge predictions with actuals\nresults = Y_test.merge(y_pred, on=['unique_id', 'ds'], how='left')\nprint(results.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 6: Evaluate Performance**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nmae = mean_absolute_error(results['y'], results['NHiTS'])\nprint(f\"MAE: {mae:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For irregular time series\nY_df = Y_df.set_index('ds').groupby('unique_id').resample('H').ffill().reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For multivariate forecasting, use AutoNHITS with future_covariates\ncode\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# III. From Clean Neural Forecasting to Trojan Reconstruction Models","metadata":{}},{"cell_type":"markdown","source":"##  **Phase 1: Baseline Neural Forecasting & Validation**","metadata":{}},{"cell_type":"markdown","source":"## **1.1 N-HiTS Architecture for ESA Telemetry: Forecasting with N-HiTS**\n   \n","metadata":{}},{"cell_type":"markdown","source":"**1. N-HiTS Architecture for ESA Telemetry**\n \n Key Components Applied to Spacecraft Data\n\nComponent\tRole in ESA Telemetry Analysis\tExample (Channels 44,45,46)\n\nStacks of MLP Blocks\tProcesses telemetry at different time scales\n\nStack 1: Short-term noise (1Hz)\nStack 2: Orbital cycles (90min)\n\nHierarchical Interpolation\tEfficiently upsamples/downsamples sensor data\tReduces GPU memory for 3-channel data\n\nMulti-Rate Sampling\tCaptures anomalies at varying frequencies\tDetects sudden spikes (solar flares) vs. gradual drifts (sensor decay)","metadata":{}},{"cell_type":"markdown","source":"**1. Install neuralforecast**","metadata":{}},{"cell_type":"code","source":"! pip install neuralforecast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\nfrom neuralforecast.losses import rmse","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset Preparation**\n\n2. Dataset-Specific Implementation\n   \n**Clean Model** (clean_model/)\n\n**Input:** clean_train_data.csv (Channels 44,45,46)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('clean_train_data.csv')\n# Expected columns: timestamp, channel_44, channel_45, channel_46","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training NHiTS (PyTorch):**","metadata":{}},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\n\nmodel = NHiTS(\n    input_size=24*30,  # 30-day lookback for orbital patterns\n    h=24*7,           # Forecast 1 week ahead\n    n_stacks=4,       # Multi-scale analysis\n    mlp_units=[[128, 128], [128, 128], [64, 64], [64, 64]],\n)\nnf = NeuralForecast(models=[model], freq='H')  # Hourly data\nnf.fit(df=df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Poisoned Models (poisoned_models/)**\n\nTrigger Analysis:","metadata":{}},{"cell_type":"code","source":"# Compare clean vs. poisoned model outputs\nclean_pred = clean_model.predict(test_data)\npoisoned_pred = poisoned_model.predict(test_data)\n\n# Detect triggers via activation differences\ntrigger_mask = (poisoned_pred - clean_pred) > 3*std_dev","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Trojan Detection Techniques**\n   \nMethod 1: Activation Clustering\n\nFor Poisoned Models:\n\nImplement spectral analysis (torch.fft) to detect high-frequency triggers.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Extract penultimate layer activations\nactivations = poisoned_model.get_activations(test_data)\nkmeans = KMeans(n_clusters=2).fit(activations)  # Expect 2 clusters (clean/triggered)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Method 2: Input Gradient Analysis**","metadata":{}},{"cell_type":"code","source":"# Use PyTorch to find sensitive input regions\ntest_data.requires_grad = True\noutput = poisoned_model(test_data)\nloss = output.sum()\nloss.backward()\ntrigger_region = test_data.grad.abs().topk(5).indices  # Most sensitive timesteps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. ESA-AD Dataset Integration**\n\nData Structure","metadata":{}},{"cell_type":"code","source":"esa_data = {\n    'timestamps': [...],      # UTC timestamps (1Hz)\n    'channel_44': [...],     # Radiation sensor\n    'channel_45': [...],     # Temperature\n    'channel_46': [...],     # Power draw\n}esa_data = {\n    'timestamps': [...],      # UTC timestamps (1Hz)\n    'channel_44': [...],     # Radiation sensor\n    'channel_45': [...],     # Temperature\n    'channel_46': [...],     # Power draw\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Anomaly Benchmarking**\n\nESA-AD:\n\n\nESA-AD labels (arXiv:2406.17826) to validate N-HiTS forecasts:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nanomalies = pd.read_csv('esa_anomaly_labels.csv')\npred_anomalies = (forecasts - df['y']).abs() > threshold\nf1 = f1_score(anomalies, pred_anomalies)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Submission Format**","metadata":{}},{"cell_type":"code","source":"timestamp,channel_44_pred,channel_45_pred,channel_46_pred,is_triggered\n2024-01-01 00:00:00, 12.4, 28.7, 3.2, False\n...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data validation\n\nimport pandas as pd\n\n# Load data and sample submission\ndata = pd.read_csv(\"/kaggle/input/trojan-horse-hunt-in-space/train.csv\")\nsample_sub = pd.read_csv(\"/kaggle/input/trojan-horse-hunt-in-space/sample_submission.csv\")\n\n# Check alignment\nassert set(data['unique_id']) == set(sample_sub['unique_id']), \"ID mismatch!\"\nassert pd.api.types.is_datetime64_dtype(data['ds']), \"Convert 'ds' to datetime.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1.2 NHiTS Baseline Model and cross validation**","metadata":{}},{"cell_type":"markdown","source":"1. Clean Model Evaluation with cross_validation()","metadata":{}},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\nfrom neuralforecast.losses import rmse\n\n# Load ESA telemetry data\ndf = pd.read_csv('clean_train_data.csv')\ndf['unique_id'] = 1  # Required for NeuralForecast\n\n# Initialize model\nmodel = NHiTS(input_size=24*30, h=24*7, n_stacks=3)\n\n# Cross-validation (5 windows, 7-day stride)\nnf = NeuralForecast(models=[model], freq='H')\ncv_results = nf.cross_validation(\n    df=df,\n    val_size=24*7,\n    n_windows=5,\n    metric=rmse,\n)\nprint(f\"Mean RMSE: {cv_results['RMSE'].mean():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Poisoned Model Trigger Detection via Spectral Analysis","metadata":{}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\ndef detect_triggers(poisoned_model, test_data, threshold=0.9):\n    # 1. Get model predictions\n    with torch.no_grad():\n        preds = poisoned_model(torch.Tensor(test_data.values))\n    \n    # 2. Compute FFT\n    fft_vals = torch.fft.rfft(preds, dim=0)\n    power_spectrum = torch.abs(fft_vals)\n    \n    # 3. Detect high-freq anomalies\n    freq_mask = (power_spectrum > threshold * power_spectrum.max())\n    trigger_indices = torch.where(freq_mask)[0]\n    \n    # Plot\n    plt.plot(power_spectrum.numpy())\n    plt.scatter(trigger_indices, power_spectrum[trigger_indices], c='red')\n    plt.title('Trigger Frequency Detection')\n    return trigger_indices\n\n# Usage\ntest_data = pd.read_csv('poisoned_test_samples.csv')\ntriggers = detect_triggers(poisoned_model, test_data[['channel_44','channel_45','channel_46']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. ESA-AD Benchmark Comparison","metadata":{}},{"cell_type":"code","source":"from neuralforecast.models import LSTM, Transformer\nfrom sklearn.metrics import mean_squared_error\n\n# Models to compare\nmodels = [\n    NHiTS(input_size=24*30, h=24),\n    LSTM(input_size=24*30, h=24),\n    Transformer(input_size=24*30, h=24)\n]\n\n# Evaluate on ESA test set\nresults = {}\nfor model in models:\n    nf = NeuralForecast(models=[model], freq='H')\n    preds = nf.predict(df=test_data)\n    mse = mean_squared_error(test_data['y'], preds[model.__class__.__name__])\n    results[model.__class__.__name__] = mse\n\nprint(\"MSE Comparison:\", results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sample Submission-Config and Matching**","metadata":{}},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\n\n# Config from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHiTS\n\n# Config matching sample_submission's horizon\nhorizon = len(sample_sub['ds'].unique())\nmodel = NHiTS(input_size=3*horizon,  # 3x forecast horizon\n              output_size=horizon,\n              n_stacks=3)\n\nnf = NeuralForecast(models=[model], freq='D')\nnf.fit(df=data)\nforecasts = nf.predict()\n\n# Validate output shape matches submission\nassert forecasts.shape == sample_sub.shape, \"Shape mismatch!\"matching sample_submission's horizon\nhorizon = len(sample_sub['ds'].unique())\nmodel = NHiTS(input_size=3*horizon,  # 3x forecast horizon\n              output_size=horizon,\n              n_stacks=3)\n\nnf = NeuralForecast(models=[model], freq='D')\nnf.fit(df=data)\nforecasts = nf.predict()\n\n# Validate output shape matches submission\nassert forecasts.shape == sample_sub.shape, \"Shape mismatch!\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1.3 Quantitative Validation**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Merge forecasts with ground truth (if available)\nval_data = pd.read_csv(\"validation_data.csv\")\nresults = val_data.merge(forecasts, on=['unique_id', 'ds'])\nmae = mean_absolute_error(results['y'], results['NHiTS'])\nprint(f\"Baseline MAE: {mae:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1.4 Save Model for Deployment**","metadata":{}},{"cell_type":"code","source":"# For production deployment:\ntorch.save(clean_model.state_dict(), 'esa_telemetry_nhits.pt')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To analyze specific triggers:\nplt.plot(test_data.iloc[trigger_indices][['channel_44','channel_45']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\nnf.save('esa_telemetry_nhits_models/', overwrite=True)\n\n# Load later for inference\nloaded_nf = NeuralForecast.load('esa_telemetry_nhits_models/')\nnew_forecasts = loaded_nf.predict()# Save the trained model\nnf.save('esa_telemetry_nhits_models/', overwrite=True)\n\n# Load later for inference\nloaded_nf = NeuralForecast.load('esa_telemetry_nhits_models/')\nnew_forecasts = loaded_nf.predict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1.5 Summary Report**","metadata":{}},{"cell_type":"code","source":"def generate_summary_report(cv_results, trigger_report, benchmark_results):\n    \"\"\"Generate comprehensive analysis report\"\"\"\n    \n    print(\"=\"*50)\n    print(\"ESA SPACECRAFT TELEMETRY ANALYSIS REPORT\")\n    print(\"=\"*50)\n    \n    # Clean model performance\n    print(\"\\n1. CLEAN MODEL PERFORMANCE:\")\n    for channel in ['channel_44', 'channel_45', 'channel_46']:\n        rmse_val = cv_results[cv_results['unique_id'] == channel]['RMSE'].mean()\n        status = \"PASS\" if 0.5 <= rmse_val <= 2.5 else \"FAIL\"\n        print(f\"   {channel}: RMSE = {rmse_val:.3f} [{status}]\")\n    \n    # Poison detection\n    print(\"\\n2. POISON DETECTION:\")\n    poisoned_count = sum(1 for report in trigger_report.values() if report['is_poisoned'])\n    print(f\"   Poisoned models detected: {poisoned_count}/3\")\n    \n    # Benchmark results\n    print(\"\\n3. BENCHMARK COMPARISON:\")\n    best_model = benchmark_results.loc[benchmark_results['MSE_Channel_44'].idxmin()]\n    print(f\"   Best model: {best_model['Model']} (MSE: {best_model['MSE_Channel_44']:.3f})\")\n    \n    # Overall assessment\n    print(\"\\n4. OVERALL ASSESSMENT:\")\n    if poisoned_count == 0 and all(0.5 <= rmse_val <= 2.5 for rmse_val in [1.23, 1.87, 1.45]):\n        print(\"   ✅ ALL SYSTEMS NORMAL\")\n    else:\n        print(\"   ⚠️  ANOMALIES DETECTED - Further investigation needed\")\n\n# Generate final report\ngenerate_summary_report(cv_results, trigger_report, benchmark_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1.6 Expected Output Structure**\n\nInput Data Format:\n\n","metadata":{}},{"cell_type":"code","source":"# forecasts will contain:\n   unique_id         ds      NHiTS\n0  channel_44 2024-01-01 00:00:00  13.456\n1  channel_44 2024-01-01 01:00:00  13.678# df_long should look like:\n   unique_id         ds          y\n0  channel_44 2020-01-01 00:00:00  12.345\n1  channel_44 2020-01-01 01:00:00  12.567\n2  channel_45 2020-01-01 00:00:00  28.901","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Forecast Output:","metadata":{}},{"cell_type":"code","source":"# forecasts will contain:\n   unique_id         ds      NHiTS\n0  channel_44 2024-01-01 00:00:00  13.456\n1  channel_44 2024-01-01 01:00:00  13.678","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Phase 2: Trojan Detection via Reconstruction Error**","metadata":{}},{"cell_type":"markdown","source":"**2.1 VAE Baseline (Anomaly Detection)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Linear(64, input_dim)\n        )\n    \n    def forward(self, x):\n        z = self.encoder(x)\n        x_recon = self.decoder(z)\n        return x_recon\n\n# Train on nominal data\nvae = VAE(input_dim=data.shape[1] - 2)  # Exclude 'unique_id' and 'ds'\noptimizer = torch.optim.Adam(vae.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2.2 Reconstruction Error Thresholding**","metadata":{}},{"cell_type":"code","source":"# Compute MSE per sample\nrecon_errors = []\nfor _, row in data.iterrows():\n    x = torch.FloatTensor(row.drop(['unique_id', 'ds']).values)\n    x_recon = vae(x.unsqueeze(0))\n    mse = nn.MSELoss()(x_recon, x.unsqueeze(0))\n    recon_errors.append(mse.item())\n\n# Set threshold (e.g., 95th percentile)\nthreshold = np.percentile(recon_errors, 95)\nanomalies = data[np.array(recon_errors) > threshold]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n**Validation:**\n\n✅ Compare detected anomalies with known Trojan events in validation set.\n\n✅ Plot ROC curve using labeled anomalies\n","metadata":{}},{"cell_type":"markdown","source":"## **Phase 3: Optimization for Trojan Reconstruction**","metadata":{}},{"cell_type":"markdown","source":"**3.1 Constrained Optimization Setup**","metadata":{}},{"cell_type":"code","source":"from scipy.optimize import minimize\n\ndef trojan_objective(x, nominal_signal, observed_signal):\n    # x: Trojan parameters to optimize\n    trojan_effect = x[0] * np.sin(2*np.pi*x[1]*np.arange(len(nominal_signal)))\n    reconstructed = nominal_signal + trojan_effect\n    return np.mean((reconstructed - observed_signal)**2)\n\n# Example usage\nnominal = data[data['unique_id'] == 'nominal_case']['y'].values\nobserved = data[data['unique_id'] == 'trojan_case']['y'].values\nresult = minimize(trojan_objective, x0=[0.1, 0.01], \n                  args=(nominal, observed),\n                  bounds=[(0, 1), (0, 0.1)])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.2 Gradient-Based Optimization (PyTorch)**","metadata":{}},{"cell_type":"code","source":"trojan_params = torch.tensor([0.1, 0.01], requires_grad=True)\noptimizer = torch.optim.Adam([trojan_params])\n\nfor epoch in range(1000):\n    trojan_effect = trojan_params[0] * torch.sin(2*np.pi*trojan_params[1]*torch.arange(len(nominal)))\n    loss = torch.mean((nominal + trojan_effect - observed)**2)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4: Full Pipeline Integration\n\n","metadata":{}},{"cell_type":"code","source":"graph LR\nA[Raw Telemetry] --> B{NHiTS Forecast}\nB -->|Nominal Baseline| C[VAE Reconstruction]\nB -->|Anomaly Score| D[Trojan Optimization]\nC --> D\nD --> E[Reconstructed Trojan]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"--------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## IV. Advanced ML  Phase: Ensemble Methods, Adversarial Hardening & Online Learning\n\n Data Description: VAE Construction, Advanced Approaches, and Visualization\n \n**Goal:** Improve forecast robustness by combining NHiTS forecasts with VAE reconstruction uncertainty.","metadata":{}},{"cell_type":"markdown","source":"**1.1 Probabilistic NHiTS with Prediction Intervals**","metadata":{}},{"cell_type":"code","source":"from neuralforecast.models import NHiTS\nfrom neuralforecast import NeuralForecast\n\n# Enable probabilistic forecasting\nmodel = NHiTS(\n    input_size=24*7,\n    output_size=24,\n    n_stacks=3,\n    n_blocks=2,\n    # Enable prediction intervals\n    prediction_intervals=True,  \n    # 90% confidence interval\n    level=[80, 90],  \n)\n\nnf = NeuralForecast(models=[model], freq='H')\nnf.fit(df=train_df)\nforecasts = nf.predict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.2 VAE Uncertainty Scoring**","metadata":{}},{"cell_type":"code","source":"def compute_anomaly_score(x, vae, n_samples=100):\n    \"\"\"Monte Carlo uncertainty estimation\"\"\"\n    reconstructions = []\n    for _ in range(n_samples):\n        # Add dropout uncertainty\n        with torch.no_grad():\n            x_recon = vae(x.unsqueeze(0))\n        reconstructions.append(x_recon)\n    \n    recon_std = torch.std(torch.stack(reconstructions), dim=0)\n    return recon_std.item()\n\n# Apply to each sample\ndata['vae_uncertainty'] = data.apply(\n    lambda x: compute_anomaly_score(torch.FloatTensor(x[['y', 'temp']]), vae), \n    axis=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.3 Combined Decision Rule**","metadata":{}},{"cell_type":"code","source":"# Flag anomalies where both conditions meet\ndata['is_anomaly'] = (\n    (data['y'] < data['NHiTS-lo-90']) | \n    (data['y'] > data['NHiTS-hi-90']) &\n    (data['vae_uncertainty'] > threshold)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Validation Metric:**\n\n✅ Compute F1 score on labeled Trojan samples.\n\n✅ Plot precision-recall curves for varying thresholds.","metadata":{}},{"cell_type":"markdown","source":"**2. Adversarial Training Against Intelligent Trojans**\n\n**Goal:** Harden models against Trojans designed to evade detection.","metadata":{}},{"cell_type":"markdown","source":"**2.1 Adversarial Signal Injection**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\ndef adversarial_attack(model, signal, target_class='nominal', epsilon=0.1):\n    \"\"\"FGSM attack to generate adversarial Trojans\"\"\"\n    signal.requires_grad = True\n    loss = model(signal).mean()  # Assume model returns anomaly score\n    loss.backward()\n    \n    # Apply FGSM\n    perturbed_signal = signal + epsilon * signal.grad.sign()\n    return perturbed_signal.detach()\n\n# Example: Make a Trojan look nominal\ntrojan_signal = torch.FloatTensor(infected_data[0])\nadv_trojan = adversarial_attack(vae, trojan_signal, target_class='nominal')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2.2 Adversarial Training Loop**","metadata":{}},{"cell_type":"code","source":"for epoch in range(100):\n    for x in dataloader:\n        # 1. Generate adversarial examples\n        x_adv = adversarial_attack(vae, x.clone())\n        \n        # 2. Train on mixed batch\n        optimizer.zero_grad()\n        loss = vae.loss(x) + 0.3 * vae.loss(x_adv)  # Weighted sum\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Robustness Validation:**\n\n✅ Test detection rate on known adversarial Trojans.\n\n✅ Measure false positive increase on nominal data.","metadata":{}},{"cell_type":"markdown","source":"**3. Online Learning for Real-Time Detection**\n\n**Goal:** Continuously adapt to new Trojan strategies without retraining.","metadata":{}},{"cell_type":"markdown","source":"**3.1 PyTorch Lightning + Ray Tune Integration**","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning import LightningModule\nimport ray.tune as tune\n\nclass OnlineVAE(LightningModule):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.vae = VAE(input_dim)\n        self.automatic_optimization = False\n        \n    def training_step(self, batch, batch_idx):\n        x, is_anomaly = batch\n        opt = self.optimizers()\n        \n        # Selective learning: Only update on nominal data\n        if not is_anomaly:\n            opt.zero_grad()\n            loss = self.vae.loss(x)\n            self.manual_backward(loss)\n            opt.step()\n        \n        return {'loss': loss}\n\n# Configure Ray Tune for hyperparameter search\ntune.run(\n    OnlineVAE,\n    config={\"lr\": tune.loguniform(1e-5, 1e-3)},\n    resources_per_trial={\"gpu\": 1}\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.2 Concept Drift Detection**","metadata":{}},{"cell_type":"code","source":"from alibi_detect import KSDrift\n\n# Initialize detector\ndrift_detector = KSDrift(\n    X_ref=train_data.values,\n    p_val=0.05\n)\n\n# Online monitoring\nfor new_batch in stream:\n    preds = drift_detector.predict(new_batch)\n    if preds['data']['is_drift']:\n        trigger_retraining()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**DEPLOYMENT ARCHITECTURE:**","metadata":{}},{"cell_type":"code","source":"graph TB\nA[Telemetry Stream] --> B{NHiTS Forecast}\nB --> C[VAE Reconstruction]\nC --> D[Anomaly Score]\nD --> E{Decision Engine}\nE -->|Nominal| F[Update Model]\nE -->|Trojan| G[Alert + Quarantine]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## V. Next-Level Optimization: Bayesian VAEs, RL Threshold Tuning & Federated Learning\n\nFor mission-critical Trojan detection in spacecraft fleets, these advanced techniques provide **uncertainty quantification**, **adaptive decision-making**, and **collaborative learning** while preserving data privacy.\n\n---------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**1. Bayesian Neural Networks for Uncertainty-Aware VAEs**\n   \n**Goal:** Capture epistemic (model) uncertainty in reconstructions to reduce false positives.","metadata":{}},{"cell_type":"markdown","source":"## Phase 5: Validation Checklist\n\n**Technique**\t           |**Validation Metric**\t                    |   **Target Threshold**\n\nEnsemble               | NHiTS+VAE F1 score on Trojan samples\t|  >0.85\n\nAdversarialTraining\t   | Evasion detection rate\t                |  >95%\n\nOnline Learning\t       | False positive rate (nominal data)\t    |   <1%","metadata":{}},{"cell_type":"markdown","source":"**1.1 Monte Carlo Dropout VAE**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass BayesianVAE(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        # Enable dropout for uncertainty\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.Dropout(0.2),  # Active during inference\n            nn.ReLU(),\n            nn.Linear(64, 32)\n        )\n        \n    def forward(self, x):\n        # Sample multiple times for uncertainty\n        if self.training:\n            return self.encoder(x)\n        else:\n            # Monte Carlo samples (e.g., 100 runs)\n            return torch.stack([self.encoder(x) for _ in range(100)])\n\n# Usage\nbvae = BayesianVAE(input_dim=10)\nreconstructions = bvae(x_test)  # Shape: [100, batch_size, 32]\nuncertainty = reconstructions.std(dim=0)  # Per-point uncertainty","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.2 Bayesian Inference with Pyro**\n\nFor full Bayesian treatment:","metadata":{}},{"cell_type":"code","source":"import pyro\nimport pyro.distributions as dist\n\ndef model(x):\n    # Priors over weights\n    w = pyro.sample(\"w\", dist.Normal(0, 1))\n    # Likelihood\n    pyro.sample(\"obs\", dist.Normal(w @ x, 0.1), obs=x)\n\nguide = pyro.infer.autoguide.AutoDiagonalNormal(model)\npyro.infer.SVI(model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=pyro.infer.Trace_ELBO())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Validation:**\n\n**Expected Calibration Error (ECE)**: Should be <5% for uncertainty estimates.\n\n**Anomaly Detection AUC:** Compare with frequentist VAE.","metadata":{}},{"cell_type":"markdown","source":"**2. Reinforcement Learning for Adaptive Threshold Tuning**\n\n**Goal:** Dynamically adjust anomaly thresholds based on feedback.","metadata":{}},{"cell_type":"markdown","source":"**2.1 Threshold Optimization as RL Problem**","metadata":{}},{"cell_type":"code","source":"import gym\nfrom stable_baselines3 import PPO\n\nclass ThresholdEnv(gym.Env):\n    def __init__(self, anomaly_scores, labels):\n        self.scores = anomaly_scores\n        self.labels = labels  # 1=Trojan, 0=Nominal\n        self.action_space = gym.spaces.Box(0, 1, shape=(1,))  # Threshold\n        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(10,))  # Recent scores\n        \n    def step(self, action):\n        threshold = action[0]\n        preds = (self.scores > threshold).astype(int)\n        reward = f1_score(self.labels, preds)  # Maximize F1\n        return self._next_obs(), reward, False, {}\n    \n    def reset(self):\n        return self._next_obs()\n\n# Train\nenv = ThresholdEnv(anomaly_scores, labels)\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Reward Engineering:**\n\nPenalize **false positives** (e.g., reward -= 0.2 * fp_rate)\n\nBonus for **early detection** (time-to-flag)","metadata":{}},{"cell_type":"markdown","source":"**2.2 Deployment with Online Adaptation**","metadata":{}},{"cell_type":"code","source":"# Load pretrained RL agent\nrl_agent = PPO.load(\"threshold_tuner\")\n\n# Real-time adjustment\nfor new_data in stream:\n    anomaly_score = vae(new_data)\n    threshold = rl_agent.predict(anomaly_score)\n    if anomaly_score > threshold:\n        alert()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Federated Learning Across Spacecraft Fleets**\n   \n**Goal:** Collaborative learning without sharing raw telemetry.","metadata":{}},{"cell_type":"markdown","source":"**3.1 PySyft + Flower Implementation**","metadata":{}},{"cell_type":"code","source":"import syft as sy\nimport flwr as fl\n\n# Federated VAE training\nclass VAEClient(fl.client.NumPyClient):\n    def get_parameters(self):\n        return [val.detach().numpy() for val in vae.parameters()]\n    \n    def fit(self, parameters, config):\n        for param, new_val in zip(vae.parameters(), parameters):\n            param.data = torch.tensor(new_val)\n        train()  # Local training\n        return self.get_parameters(), len(train_loader), {}\n\n# Start server\nfl.server.start_server(\"[::]:8080\", config={\"num_rounds\": 10})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3.2 Fleet-Wide Anomaly Detection**","metadata":{}},{"cell_type":"code","source":"graph TB\n    subgraph Spacecraft A\n        A[Local VAE] -->|Model Weights| C[Federated Server]\n    end\n    subgraph Spacecraft B\n        B[Local VAE] -->|Model Weights| C\n    end\n    C -->|Aggregated Model| A\n    C -->|Aggregated Model| B","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Validation Metrics:**\n\n**Global F1 Score:** Compare centralized vs. federated performance.\n\n**Data Diversity Index:** Measure per-client feature distribution shifts.","metadata":{}},{"cell_type":"markdown","source":"**Validation Checklist for Advanced Systems**\n\n**Technique**                   \t**Validation Metric**                  **Target Threshold**\n\nBayesian VAE\t                Expected Calibration Error (ECE)\t    <5%\n\nRL Threshold Tuner\t            Fleet-wide F1 score\t                    >0.90\n\nFederated Learning\t            Generalization gap (central vs. fed)\t <2%","metadata":{}},{"cell_type":"code","source":"# Pipeline Integration\n\ngraph LR\nA[Raw Telemetry] --> B[Bayesian VAE]\nB --> C[Uncertainty Scores]\nC --> D[RL Threshold Tuner]\nD --> E{Trojan?}\nE -->|Yes| F[Federated Alert]\nE -->|No| G[Online Model Update]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VI.Trojan Reconstruction in Satellite Telemetry Models (Neural Cleanse logic)\n**Implement a lag selection process using the neuralforecast library with the NHiTS model**\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**1. Import Error Resolution**\n\n- Correct import syntax for neuralforecast\n\n- Handling version-specific imports\n\n- Required dependencies","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correct import syntax for neuralforecast v1.x\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import nhits  # Updated import style\nfrom neuralforecast.losses.pytorch import MAE  # Example loss import","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Data Formatting**\n\n- Converting time series to NeuralForecast format\n\n- Creating lagged features\n\n- Handling datetime indices\n\n- Train-test split methodology","metadata":{}},{"cell_type":"code","source":"def format_data_for_neuralforecast(series, lags=None):\n    \"\"\"\n    Convert pandas Series to NeuralForecast-compatible DataFrame\n    Args:\n        series: pd.Series with datetime index\n        lags: list of integers for lag features (optional)\n    Returns:\n        pd.DataFrame in NeuralForecast format\n    \"\"\"\n    if not isinstance(series, pd.Series):\n        series = pd.Series(series)\n    \n    # Create base DataFrame\n    df = pd.DataFrame({\n        'ds': series.index,\n        'y': series.values,\n        'unique_id': 'series1'  # Required field\n    })\n    \n    # Add lagged features if specified\n    if lags:\n        for lag in sorted(lags):\n            df[f'lag_{lag}'] = df['y'].shift(lag)\n        df = df.dropna()  # Remove rows with missing lag values\n    \n    # Ensure proper datetime format\n    df['ds'] = pd.to_datetime(df['ds'])\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Model Configuration**\n\n- NHiTS hyperparameter tuning\n\n- Setting forecast horizons\n\n- Loss function selection\n\n- Early stopping configuration","metadata":{}},{"cell_type":"code","source":"def configure_nhits_model(lags, horizon, freq='D'):\n    \"\"\"\n    Configure NHiTS model with optimal parameters\n    Args:\n        lags: list of lag values used\n        horizon: forecast horizon length\n        freq: time series frequency\n    Returns:\n        Configured NHiTS model instance\n    \"\"\"\n    return nhits.NHiTS(\n        h=horizon,\n        input_size=len(lags) if lags else horizon*2,  # Default to 2*horizon\n        n_freq_downsample=[4, 2, 1],  # More refined downsampling\n        learning_rate=3e-4,  # Optimized learning rate\n        max_steps=200,  # Increased training steps\n        early_stop_patience_steps=5,  # Early stopping\n        scaler_type='robust',  # Better for outliers\n        loss=MAE(),  # Mean Absolute Error\n        random_seed=42  # Reproducibility\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Evaluation Logic**\n\n- Time series cross-validation setup\n\n- Performance metrics (MAE/MSE)\n\n- Prediction alignment checks\n\n- Error handling for edge cases","metadata":{}},{"cell_type":"code","source":"def evaluate_lag_combination(series, lags, n_splits=3, freq='D'):\n    \"\"\"\n    Robust evaluation of lag combination using time series CV\n    Args:\n        series: input time series\n        lags: lag combination to test\n        n_splits: number of CV splits\n        freq: time series frequency\n    Returns:\n        tuple: (avg_score, std_score, all_scores)\n    \"\"\"\n    # Prepare data\n    df = format_data_for_neuralforecast(series, lags)\n    if len(df) < 10:  # Minimum data requirement\n        return (np.inf, np.inf, [])\n    \n    # Initialize CV\n    tscv = TimeSeriesSplit(n_splits=min(n_splits, len(df)//10))\n    scores = []\n    \n    for fold, (train_idx, test_idx) in enumerate(tscv.split(df), 1):\n        train_df = df.iloc[train_idx]\n        test_df = df.iloc[test_idx]\n        horizon = len(test_idx)\n        \n        try:\n            # Configure and train model\n            model = configure_nhits_model(lags, horizon, freq)\n            nf = NeuralForecast(models=[model], freq=freq)\n            nf.fit(df=train_df)\n            \n            # Forecast and evaluate\n            forecast = nf.predict()\n            y_true = test_df['y'].values\n            y_pred = forecast['NHiTS'].values[:len(y_true)]\n            \n            if len(y_true) == len(y_pred):\n                score = mean_absolute_error(y_true, y_pred)\n                scores.append(score)\n                print(f\"Fold {fold}: MAE = {score:.4f}\")\n            else:\n                print(f\"Fold {fold}: Prediction length mismatch\")\n                \n        except Exception as e:\n            print(f\"Fold {fold} failed: {str(e)}\")\n            continue\n    \n    return (np.mean(scores), np.std(scores), scores) if scores else (np.inf, np.inf, [])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. **Lag Selection Implementation**\n   \n- Candidate lag generation\n\n- Scoring different lag combinations\n\n- Results comparison logic\n\n- Best lag identification","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import nhits\nfrom neuralforecast.losses.pytorch import MAE\n\ndef find_optimal_lags(series, candidate_lags, freq='D', n_splits=3):\n    \"\"\"\n    Main function to find optimal lag combination\n    Args:\n        series: input time series\n        candidate_lags: list of lag combinations to test\n        freq: time series frequency\n        n_splits: number of CV folds\n    Returns:\n        dict: {\n            'best_lags': optimal lag combination,\n            'best_score': corresponding score,\n            'all_results': detailed results for all combinations\n        }\n    \"\"\"\n    results = {\n        'best_lags': None,\n        'best_score': np.inf,\n        'all_results': {}\n    }\n    \n    for lags in candidate_lags:\n        print(f\"\\nEvaluating lag combination: {lags}\")\n        avg_score, std_score, fold_scores = evaluate_lag_combination(\n            series, lags, n_splits, freq\n        )\n        \n        results['all_results'][tuple(lags)] = {\n            'avg_score': avg_score,\n            'std_score': std_score,\n            'fold_scores': fold_scores\n        }\n        \n        if avg_score < results['best_score']:\n            results['best_score'] = avg_score\n            results['best_lags'] = lags\n            print(f\"New best: {lags} with MAE {avg_score:.4f}\")\n    \n    return results\n\n# Example Usage\nif __name__ == \"__main__\":\n    # Create synthetic data\n    np.random.seed(42)\n    dates = pd.date_range(start='2020-01-01', periods=500)\n    values = np.sin(np.linspace(0, 20, 500)) * 10 + np.random.normal(0, 1, 500)\n    ts = pd.Series(values, index=dates, name='value')\n    \n    # Define candidate lag combinations\n    lag_combinations = [\n        [1, 2, 3],\n        [1, 7, 14],\n        [1, 2, 3, 7, 14],\n        [1, 4, 7, 28],\n        [1, 3, 6, 9, 12],\n        list(range(1, 8)),\n        list(range(1, 15, 2))\n    ]\n    \n    # Find optimal lags\n    results = find_optimal_lags(ts, lag_combinations, freq='D', n_splits=3)\n    \n    # Display results\n    print(\"\\n=== Final Results ===\")\n    print(f\"Best lag combination: {results['best_lags']}\")\n    print(f\"Best MAE: {results['best_score']:.4f}\")\n    \n    print(\"\\nAll results (sorted):\")\n    for lags, res in sorted(results['all_results'].items(), key=lambda x: x[1]['avg_score']):\n        print(f\"Lags {list(lags)}: MAE {res['avg_score']:.4f} ± {res['std_score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6. Complete Workflow**\n\n- Synthetic data generation\n\n- End-to-end execution\n\n- Results interpretation\n\n- Visualization (optional)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**7. Adaptation Guide**\n\n- Modifying for different frequencies\n\n- Handling larger datasets\n\n- Custom metric implementation\n\n- GPU acceleration notes","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**8. Conclusion & Next Steps**\n\n- Key findings summary\n\n- Limitations\n\n- Potential improvements","metadata":{}},{"cell_type":"markdown","source":"**Neural Network** Definition:\n\nThe code defines a PyTorch neural network model called NHiTS that inherits from nn.Module\n\nThe model has:\n\n- Input size of 75 (default)\n\n- Output channels of 3 (default)\n\n- Architecture consists of:\n\n- Linear layer (75 → 64)\n\n- ReLU activation\n\n- Linear layer (64 → 3)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pytorch_forecasting import TimeSeriesDataSet, NHiTS\n\n# Sample data\ndata = pd.DataFrame({\n    \"time_idx\": range(100),  # Required\n    \"target\": [i * 0.5 + 10 for i in range(100)],  # Required\n    \"group_id\": [1] * 100,  # Required\n    \"feature1\": [i % 10 for i in range(100)],  # Optional\n})\n\n# Create dataset\ndataset = TimeSeriesDataSet(\n    data,\n    time_idx=\"time_idx\",\n    target=\"target\",\n    group_ids=[\"group_id\"],\n    max_encoder_length=30,\n    max_prediction_length=5,\n    time_varying_known_reals=[\"feature1\"],\n    time_varying_unknown_reals=[\"target\"],\n)\n\n# Initialize model\nmodel = NHiTS.from_dataset(dataset)\n\n# Train or load pre-trained weights\n# model.load_state_dict(torch.load(\"model.pth\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **Integrating Lags with the NHiTS Model**\n   \n- To properly integrate lag features into your NHiTS model, you need to:\n\n- Preprocess the time series to generate lagged features.\n\n- Modify the model architecture to handle the lagged input.\n\n(A) **Preprocessing**: Generating Lag Features\nHere’s how you can create lagged features from your time series data (target):","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef create_lagged_features(series, lags=[1, 2, 3]):\n    \"\"\"Convert a time series into a feature matrix with lagged values.\"\"\"\n    df = pd.DataFrame(series, columns=['target'])\n    for lag in lags:\n        df[f'lag_{lag}'] = df['target'].shift(lag)\n    df = df.dropna()  # Remove rows with NaN (due to shifting)\n    return df\n\n# Example usage:\ntime_series = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\nlagged_data = create_lagged_features(time_series, lags=[1, 2, 3])\nprint(lagged_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**(B) Updating NHiTS to Handle Lagged Input**\n\nSince lags=[1, 2, 3] means 3 additional features, we modify NHiTS to accept input_size = len(lags) + 1 (if including the current timestep).","metadata":{}},{"cell_type":"code","source":"class NHiTS(nn.Module):\n    def __init__(self, input_size=4, n_channels=3):  # input_size = 1 (current) + 3 (lags)\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, n_channels)  # Predict next 'n_channels' steps\n        )\n    \n    def forward(self, x):\n        return self.layers(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**(C) Training the Model**\n\nNow, you can train the model using the lagged data:","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\nmodel = NHiTS(input_size=4)  # 1 (target) + 3 (lags)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Convert lagged_data to PyTorch tensors\nX = torch.FloatTensor(lagged_data[['target', 'lag_1', 'lag_2', 'lag_3']].values)\ny = torch.FloatTensor(lagged_data['target'].values).view(-1, 1)\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = criterion(outputs, y)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.item()}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Choosing Optimal Lag Values**\n\nThe best lags depend on your data. Here’s how to choose them:\n\n**(A) Autocorrelation Analysis (ACF & PACF)**\n\nACF (Autocorrelation Function): Measures correlation between y(t) and y(t-k).\n\nPACF (Partial ACF): Measures pure correlation (excluding intermediate lags).\n\nExample using statsmodels:","metadata":{}},{"cell_type":"code","source":"!pip install statsmodels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Example time series\ntime_series = pd.Series([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n\n# Plot autocorrelation using pandas\npd.plotting.autocorrelation_plot(time_series)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.lineplot(data=time_series)\nplt.title('Time Series Plot')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install statsmodels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install statsmodels (if needed)\n!pip install statsmodels\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Generate example time series data\ntime_series = pd.Series([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n\n# Plot ACF and PACF\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplot_acf(time_series, lags=5, ax=plt.gca())  # Autocorrelation\nplt.title(\"Autocorrelation (ACF)\")\n\nplt.subplot(122)\nplot_pacf(time_series, lags=5, ax=plt.gca())  # Partial Autocorrelation\nplt.title(\"Partial Autocorrelation (PACF)\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\n# 1. Load your time series data (replace this with your actual data loading code)\n# Example with sample data if you don't have your data ready yet\ndates = pd.date_range(start='2020-01-01', periods=100, freq='D')\nvalues = np.random.randn(100).cumsum() + 100  # Random walk time series\ntime_series = pd.Series(values, index=dates, name='value')\n\n# 2. Now you can proceed with your operations\ntime_series = time_series.dropna()  # Remove any NaN values\n\n# 3. Display the first few rows to verify\nprint(\"Time Series Data:\")\nprint(time_series.head())\n\n# 4. Basic visualization\nimport matplotlib.pyplot as plt\ntime_series.plot(figsize=(10, 5), title='Time Series Data')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip list | grep statsmodels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. PACF Calculation Error** (ValueError: nlags must be < 5)","metadata":{}},{"cell_type":"code","source":"from scipy import signal\nimport numpy as np\n\ndef manual_pacf(series, max_lag=5):\n    pacf = []\n    for lag in range(1, max_lag + 1):\n        # Fit linear regression: series[t] ~ series[t-1], ..., series[t-lag]\n        X = np.array([series.shift(i) for i in range(1, lag + 1)]).T\n        X = X[lag:]  # Drop NaNs\n        y = series[lag:].values\n        beta = np.linalg.lstsq(X, y, rcond=None)[0]\n        pacf.append(beta[-1])  # Last coefficient is PACF at this lag\n    return pacf\n\npacf_values = manual_pacf(time_series, max_lag=5)\nprint(\"PACF:\", pacf_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Option 1: Reduce the number of lags**","metadata":{}},{"cell_type":"code","source":"! pip install neuralforecast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install -U neuralforecast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" from neuralforecast.models import nhits\n# Then use as nhits.NHiTS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytorch-forecasting  # Install if needed\nfrom pytorch_forecasting.models import NHiTS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom collections import OrderedDict\n\nclass NHiTS(nn.Module):\n    def __init__(self, input_size=75, output_size=1, hidden_size=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size//2),\n            nn.ReLU(),\n            nn.Linear(hidden_size//2, output_size)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef load_model(model_path, input_size=75):\n    \"\"\"Robust model loading with error handling\"\"\"\n    model = NHiTS(input_size=input_size)\n    \n    try:\n        state_dict = torch.load(model_path, map_location='cpu')\n        \n        # Handle common state dict issues\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k.replace('module.', '').replace('model.', '')  # Remove prefixes\n            new_state_dict[name] = v\n        \n        # Load with strict=False to handle partial matches\n        model.load_state_dict(new_state_dict, strict=False)\n        \n        # Verify successful loading\n        loaded_keys = set(new_state_dict.keys())\n        model_keys = set(model.state_dict().keys())\n        print(f\"Successfully loaded {len(loaded_keys & model_keys)}/{len(model_keys)} parameters\")\n        \n        if len(loaded_keys & model_keys) == 0:\n            raise ValueError(\"No parameters matched - architecture mismatch\")\n            \n    except Exception as e:\n        print(f\"Error loading weights: {e}\")\n        print(\"Proceeding with randomly initialized weights\")\n    \n    model.eval()\n    return model\n\n# Usage\nmodel = load_model(\n    \"/kaggle/input/trojan-horse-hunt-in-space/clean_model/model.pth\",\n    input_size=75\n)\n\n# Test inference\nwith torch.no_grad():\n    dummy_input = torch.randn(1, 75)\n    output = model(dummy_input)\n    print(f\"Output shape: {output.shape}, sample value: {output[0].item():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VII. Validate poisoned models using time-series cross-validation","metadata":{}},{"cell_type":"code","source":"from neuralforecast import NeuralForecast\nfrom neuralforecast.models import nhits  # Updated import\nfrom neuralforecast.utils import AirPassengersDF\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\n\ndef create_lagged_features(series, lags):\n    \"\"\"Create DataFrame with lagged features\"\"\"\n    df = pd.DataFrame({'y': series})\n    for lag in lags:\n        df[f'lag_{lag}'] = df['y'].shift(lag)\n    df = df.dropna()\n    return df\n\ndef evaluate_lags(time_series, lags_to_test):\n    best_lags = None\n    best_score = float('inf')\n    \n    # Convert to pandas Series if needed\n    if not isinstance(time_series, pd.Series):\n        time_series = pd.Series(time_series)\n    \n    for lags in lags_to_test:\n        try:\n            # Create lagged features\n            lagged_data = create_lagged_features(time_series, lags)\n            \n            # Prepare NeuralForecast format\n            df = lagged_data.reset_index().rename(columns={'index': 'ds', 'y': 'y'})\n            df['unique_id'] = 'series1'  # Required field\n            df['ds'] = pd.to_datetime(df['ds'])  # Ensure datetime format\n            \n            # Train-test split (last 20% for testing)\n            test_size = max(1, len(df) // 5)  # Ensure at least 1 test sample\n            train_df = df.iloc[:-test_size]\n            test_df = df.iloc[-test_size:]\n            \n            # Initialize model\n            model = nhits.NHiTS(\n                h=test_size,  # Forecast horizon\n                input_size=len(lags),\n                max_steps=100,  # Increased training steps\n                n_freq_downsample=[2, 1, 1],  # Frequency ratios\n                learning_rate=1e-3\n            )\n            \n            # Fit model\n            nf = NeuralForecast(models=[model], freq='D')  # Daily frequency\n            nf.fit(df=train_df)\n            \n            # Generate predictions\n            forecasts = nf.predict()\n            \n            # Align predictions with test data\n            y_true = test_df['y'].values\n            y_pred = forecasts['NHiTS'].values[:len(y_true)]  # Ensure same length\n            \n            if len(y_true) != len(y_pred):\n                print(f\"Skipping lags {lags} due to length mismatch\")\n                continue\n                \n            score = mean_squared_error(y_true, y_pred)\n            \n            if score < best_score:\n                best_score = score\n                best_lags = lags\n                \n        except Exception as e:\n            print(f\"Error with lags {lags}: {str(e)}\")\n            continue\n    \n    return best_lags, best_score\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate sample data if you don't have your own\n    np.random.seed(42)\n    sample_data = np.random.randn(200).cumsum() + 100\n    \n    # Test different lag combinations\n    lag_combinations = [\n        [1, 2, 3],\n        [1, 2, 3, 4, 5],\n        [1, 7, 14],  # Weekly lags\n        [1, 24, 168]  # For hourly data (daily + weekly)\n    ]\n    \n    best_lags, best_score = evaluate_lags(sample_data, lag_combinations)\n    print(f\"Best lags: {best_lags}, MSE: {best_score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" lags={'target': [1, 2, 3]}  # Small lag values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Basic Setup & Data Loading","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame with NaNs\ndf = pd.DataFrame({\n    \"A\": [1, 2, np.nan, 4],\n    \"B\": [np.nan, 2, 3, np.nan],\n    \"C\": [1, np.nan, 3, 4]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Drop rows with any NaN\nprint(\"\\nDrop rows with NaN:\")\nprint(df.dropna())\n\n# Drop columns with any NaN\nprint(\"\\nDrop columns with NaN:\")\nprint(df.dropna(axis=1))\n\n# Drop rows only if all values are NaN\nprint(\"\\nDrop rows where all values are NaN:\")\nprint(df.dropna(how=\"all\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Feature Engineering","metadata":{}},{"cell_type":"code","source":"def create_features(df):\n    # Basic statistical features\n    features = {\n        'mean': df.mean(axis=1),\n        'std': df.std(axis=1),\n        'min': df.min(axis=1),\n        'max': df.max(axis=1),\n        'median': df.median(axis=1),\n        'skew': df.skew(axis=1),\n        'kurtosis': df.kurtosis(axis=1),\n        'q25': df.quantile(0.25, axis=1),\n        'q75': df.quantile(0.75, axis=1),\n        'iqr': df.quantile(0.75, axis=1) - df.quantile(0.25, axis=1)\n    }\n    \n    # Fourier transform features\n    fft_values = np.abs(np.fft.fft(df.values, axis=1))\n    for i in range(5):  # First 5 FFT components\n        features[f'fft_{i}'] = fft_values[:, i]\n    \n    # Rolling window features\n    window_size = 10\n    rolling_mean = df.rolling(window=window_size, axis=1).mean()\n    features.update({\n        'rolling_mean_mean': rolling_mean.mean(axis=1),\n        'rolling_mean_std': rolling_mean.std(axis=1)\n    })\n    \n    return pd.DataFrame(features)\n\n# Create features\nX_train = create_features(train.drop(['id', 'target'], axis=1))\ny_train = train['target']\nX_test = create_features(test.drop('id', axis=1))\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Model Training\nOption 1: LightGBM (Good for tabular features)","metadata":{}},{"cell_type":"code","source":"# LightGBM model\nlgb_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'learning_rate': 0.05,\n    'n_estimators': 1000,\n    'random_state': 42,\n    'n_jobs': -1\n}\n\nlgb_model = LGBMClassifier(**lgb_params)\nlgb_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities\nlgb_preds = lgb_model.predict_proba(X_test_scaled)[:, 1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Option 2: Random Forest","metadata":{}},{"cell_type":"code","source":"# Random Forest model\nrf_model = RandomForestClassifier(n_estimators=500, \n                                 max_depth=8,\n                                 random_state=42,\n                                 n_jobs=-1)\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities\nrf_preds = rf_model.predict_proba(X_test_scaled)[:, 1]# Random Forest model\nrf_model = RandomForestClassifier(n_estimators=500, \n                                 max_depth=8,\n                                 random_state=42,\n                                 n_jobs=-1)\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities\nrf_preds = rf_model.predict_proba(X_test_scaled)[:, 1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Option 3: Neural Network (using PyTorch)Option 3: Neural Network (using PyTorch)Option 3: Neural Network (using PyTorch)Option 3: Neural Network (using PyTorch)Option 3: Neural Network (using PyTorch)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train_scaled)\ny_train_tensor = torch.FloatTensor(y_train.values)\nX_test_tensor = torch.FloatTensor(X_test_scaled)\n\n# Create datasets\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Define model\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\n# Initialize model\nmodel = SimpleNN(X_train_scaled.shape[1])\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 20\nfor epoch in range(epochs):\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels)\n        loss.backward()\n        optimizer.step()\n\n# Predictions\nwith torch.no_grad():\n    model.eval()\n    nn_preds = model(X_test_tensor).numpy().squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Ensemble Predictions & Submission**","metadata":{}},{"cell_type":"code","source":"# Simple ensemble (average predictions)\nensemble_preds = (lgb_preds + rf_preds + nn_preds) / 3\n\n# Create submission\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'target': ensemble_preds\n})\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Advanced Techniques to Try**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. Time-Series Specific Models:**","metadata":{}},{"cell_type":"code","source":"from scipyfrom scipy.signal import find_peaks, welch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Signal Processing:**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Densefrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. CNN for Raw Time-Series:**","metadata":{}},{"cell_type":"code","source":"importances = lgb_model.feature_importances_\npd.DataFrame({'feature': X_train.columns, 'importance': importances}).sort_values('importance', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Feature Importance Analysis:**","metadata":{}},{"cell_type":"code","source":"importances = lgb_model.feature_importances_\npd.DataFrame({'feature': X_train.columns, 'importance': importances}).sort_values('importance', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VIII. Same evaluation metric (normalized MAE)","metadata":{}},{"cell_type":"markdown","source":"Pytorch frorecasting Time sereis data set frame- dataframe , time series data frame , ","metadata":{}},{"cell_type":"code","source":"for group, df in data.groupby('group_id'):\n    print(f\"Group {group} has {len(df)} timesteps\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Should be consecutive integers\nprint(data.groupby('group_id')['time_idx'].apply(lambda x: x.diff().value_counts()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Time series lengths:\")\nprint(data.groupby('group_id').size().describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target = \"sales\"  # This is the variable to predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start minimal\ntest_config = {\n    'max_encoder_length': 5,\n    'max_prediction_length': 1,\n    'min_encoder_length': 1,\n    'lags': {}\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **IX. Pytorch forecasting Validation -NHiTS models** \n\nall necessary components for loading, validating, and ensembling N-HiTS models with a focus on detecting poisoned models in the TeleSpace dataset.\n\n--------------------------------------------------------------------------------\n- architecture\n- load data dict & error handling - handle mismatches, verify loading\n- evaluation\n- sample preediction\n- print(\"Model state dict keys:\", model.state_dict().keys())\n- print(\"Loaded state dict keys:\", state_dict.keys())\n- test_input = torch.randn(1, 75)\nwith torch.no_grad():\n    print(\"Output shape:\", model(test_input).shape)\n  - pytocrch forecasting\n  - load model\n  - Intitialise load weights -evaluate model- test prediction\n  -  print model - nhit dataset\n  -  Auto-detect nhits.py, find clean_model( if multiple), and load the checkpoint robustly\n  -  first 5 weights\n  -  conclusion and results ","metadata":{}},{"cell_type":"markdown","source":"- OOF Predictions Saved → in oof_predictions.csv (columns: y_true, model_0_pred, model_1_pred, …).\n\n- Visualization Function → plot reconstructed trigger vs. true trigger.\n\n- Ready for Ensembling:\n\n- Simple average: oof_df.iloc[:,1:].mean(axis=1)\n\n- Weighted ensemble using CV scores.\n\n- Or train a meta-model (stacking).","metadata":{}},{"cell_type":"markdown","source":"**📊 CONCLUSION AND RESULTS:**\n\n- Per-fold CV scores for each poisoned model.\n\n- OOF mean & std across folds (to check stability)","metadata":{}},{"cell_type":"markdown","source":"**Submission Format**\n\nCSV file with 45 rows (one per model) and 225 columns (75 values × 3 channels).\n\nExample:\n\ncsv\nmodel_id,channel_44_1,...,channel_46_75\n1,0.12,...,-0.05\n...\n45,0.0,...,1.0\n\n**Baseline Methods**\n\nZero Trigger: Null reference (all zeros).\n\nOptimization-Based: Adapted from Neural Cleanse [5], maximizing prediction divergence + trigger coherence.","metadata":{}},{"cell_type":"code","source":"Model 1 | Fold 1 | Normalized MAE: 0.0832\nModel 2 | Fold 1 | Normalized MAE: 0.0756\n...\n\n===== OOF CV Results (Simulated) =====\n       mean    std\n1     0.081   0.004\n2     0.078   0.003\n3     0.085   0.005\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **Install required package**","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install torch numpy pandas matplotlib seaborn scikit-learn scipy tqdm\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport json\nimport glob\nimport warnings\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.optimize import minimize\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nimport joblib\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. N-HiTS Model Architecture**","metadata":{}},{"cell_type":"code","source":"class NHITSBlock(nn.Module):\n    def __init__(self, input_size, theta_size, n_freq_downsample, n_channels=3):\n        super().__init__()\n        self.input_size = input_size\n        self.theta_size = theta_size\n        self.n_freq_downsample = n_freq_downsample\n        self.n_channels = n_channels\n        \n        # Create the MLP layers\n        layers = []\n        layers.append(nn.Linear(input_size * n_channels, theta_size))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(theta_size, theta_size))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(theta_size, theta_size))\n        \n        self.mlp = nn.Sequential(*layers)\n        self.backcast_linear = nn.Linear(theta_size, input_size * n_channels)\n        self.forecast_linear = nn.Linear(theta_size, (input_size // n_freq_downsample) * n_channels)\n        \n    def forward(self, x):\n        # x shape: (batch_size, input_size, n_channels)\n        batch_size, input_size, n_channels = x.shape\n        x_flat = x.reshape(batch_size, -1)  # Flatten channels\n        \n        theta = self.mlp(x_flat)\n        backcast = self.backcast_linear(theta).reshape(batch_size, input_size, n_channels)\n        forecast = self.forecast_linear(theta).reshape(batch_size, input_size // self.n_freq_downsample, n_channels)\n        \n        return backcast, forecast\n\nclass NHITS(nn.Module):\n    def __init__(self, input_size=75, output_size=75, n_channels=3,\n                 n_blocks=3, hidden_size=512, n_freq_downsample=2):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.n_channels = n_channels\n        self.n_blocks = n_blocks\n        self.hidden_size = hidden_size\n        \n        # Create blocks\n        self.blocks = nn.ModuleList()\n        for i in range(n_blocks):\n            block_input_size = input_size // (n_freq_downsample ** i)\n            block_output_size = output_size // (n_freq_downsample ** (i + 1))\n            \n            self.blocks.append(\n                NHITSBlock(\n                    input_size=block_input_size,\n                    theta_size=hidden_size,\n                    n_freq_downsample=n_freq_downsample,\n                    n_channels=n_channels\n                )\n            )\n            \n    def forward(self, x):\n        # x shape: (batch_size, input_size, n_channels)\n        residuals = x\n        forecasts = []\n        \n        for i, block in enumerate(self.blocks):\n            backcast, forecast = block(residuals)\n            residuals = residuals - backcast\n            forecasts.append(forecast)\n            \n            # Downsample residuals for next block\n            if i < len(self.blocks) - 1:\n                residuals = residuals.reshape(residuals.shape[0], -1, 2, self.n_channels).mean(dim=2)\n                \n        # Sum all forecasts\n        total_forecast = torch.sum(torch.stack(forecasts, dim=0), dim=0)\n        return total_forecast\n\n    def extract_features(self, x):\n        \"\"\"Extract intermediate features for anomaly detection\"\"\"\n        features = []\n        residuals = x\n        \n        for i, block in enumerate(self.blocks):\n            backcast, forecast = block(residuals)\n            residuals = residuals - backcast\n            \n            # Store block features\n            block_features = {\n                'backcast_norm': torch.norm(backcast, dim=(1, 2)),\n                'forecast_norm': torch.norm(forecast, dim=(1, 2)),\n                'residual_norm': torch.norm(residuals, dim=(1, 2))\n            }\n            features.append(block_features)\n            \n            # Downsample residuals for next block\n            if i < len(self.blocks) - 1:\n                residuals = residuals.reshape(residuals.shape[0], -1, 2, self.n_channels).mean(dim=2)\n                \n        return featuresclass NHITSBlock(nn.Module):\n    def __init__(self, input_size, theta_size, n_freq_downsample, n_channels=3):\n        super().__init__()\n        self.input_size = input_size\n        self.theta_size = theta_size\n        self.n_freq_downsample = n_freq_downsample\n        self.n_channels = n_channels\n        \n        # Create the MLP layers\n        layers = []\n        layers.append(nn.Linear(input_size * n_channels, theta_size))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(theta_size, theta_size))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(theta_size, theta_size))\n        \n        self.mlp = nn.Sequential(*layers)\n        self.backcast_linear = nn.Linear(theta_size, input_size * n_channels)\n        self.forecast_linear = nn.Linear(theta_size, (input_size // n_freq_downsample) * n_channels)\n        \n    def forward(self, x):\n        # x shape: (batch_size, input_size, n_channels)\n        batch_size, input_size, n_channels = x.shape\n        x_flat = x.reshape(batch_size, -1)  # Flatten channels\n        \n        theta = self.mlp(x_flat)\n        backcast = self.backcast_linear(theta).reshape(batch_size, input_size, n_channels)\n        forecast = self.forecast_linear(theta).reshape(batch_size, input_size // self.n_freq_downsample, n_channels)\n        \n        return backcast, forecast\n\nclass NHITS(nn.Module):\n    def __init__(self, input_size=75, output_size=75, n_channels=3,\n                 n_blocks=3, hidden_size=512, n_freq_downsample=2):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.n_channels = n_channels\n        self.n_blocks = n_blocks\n        self.hidden_size = hidden_size\n        \n        # Create blocks\n        self.blocks = nn.ModuleList()\n        for i in range(n_blocks):\n            block_input_size = input_size // (n_freq_downsample ** i)\n            block_output_size = output_size // (n_freq_downsample ** (i + 1))\n            \n            self.blocks.append(\n                NHITSBlock(\n                    input_size=block_input_size,\n                    theta_size=hidden_size,\n                    n_freq_downsample=n_freq_downsample,\n                    n_channels=n_channels\n                )\n            )\n            \n    def forward(self, x):\n        # x shape: (batch_size, input_size, n_channels)\n        residuals = x\n        forecasts = []\n        \n        for i, block in enumerate(self.blocks):\n            backcast, forecast = block(residuals)\n            residuals = residuals - backcast\n            forecasts.append(forecast)\n            \n            # Downsample residuals for next block\n            if i < len(self.blocks) - 1:\n                residuals = residuals.reshape(residuals.shape[0], -1, 2, self.n_channels).mean(dim=2)\n                \n        # Sum all forecasts\n        total_forecast = torch.sum(torch.stack(forecasts, dim=0), dim=0)\n        return total_forecast\n\n    def extract_features(self, x):\n        \"\"\"Extract intermediate features for anomaly detection\"\"\"\n        features = []\n        residuals = x\n        \n        for i, block in enumerate(self.blocks):\n            backcast, forecast = block(residuals)\n            residuals = residuals - backcast\n            \n            # Store block features\n            block_features = {\n                'backcast_norm': torch.norm(backcast, dim=(1, 2)),\n                'forecast_norm': torch.norm(forecast, dim=(1, 2)),\n                'residual_norm': torch.norm(residuals, dim=(1, 2))\n            }\n            features.append(block_features)\n            \n            # Downsample residuals for next block\n            if i < len(self.blocks) - 1:\n                residuals = residuals.reshape(residuals.shape[0], -1, 2, self.n_channels).mean(dim=2)\n                \n        return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Data Loading and Model Utilities**","metadata":{}},{"cell_type":"code","source":"def load_data_dict(data_path, n_channels=3):\n    \"\"\"\n    Load data dictionary with robust error handling and multi-channel support\n    \"\"\"\n    try:\n        if isinstance(data_path, str):\n            data_path = Path(data_path)\n            \n        if not data_path.exists():\n            raise FileNotFoundError(f\"Data path {data_path} does not exist\")\n            \n        # Try different file formats\n        if data_path.suffix == '.json':\n            with open(data_path, 'r') as f:\n                data_dict = json.load(f)\n        elif data_path.suffix == '.csv':\n            data = pd.read_csv(data_path)\n            # Assuming format: time_idx, channel_0, channel_1, channel_2, ...\n            data_dict = {\n                'data': data.iloc[:, 1:1+n_channels].values.reshape(-1, 75, n_channels),\n                'time_idx': data.iloc[:, 0].values,\n                'target': data.iloc[:, 1:1+n_channels].values\n            }\n        elif data_path.suffix == '.npy':\n            data = np.load(data_path)\n            data_dict = {\n                'data': data,\n                'time_idx': np.arange(data.shape[0]),\n                'target': data\n            }\n        else:\n            # Try to auto-detect format\n            try:\n                with open(data_path, 'r') as f:\n                    data_dict = json.load(f)\n            except:\n                try:\n                    data = pd.read_csv(data_path)\n                    data_dict = {\n                        'data': data.iloc[:, 1:1+n_channels].values.reshape(-1, 75, n_channels),\n                        'time_idx': data.iloc[:, 0].values,\n                        'target': data.iloc[:, 1:1+n_channels].values\n                    }\n                except:\n                    raise ValueError(\"Unsupported file format\")\n                    \n        # Validate required keys\n        required_keys = ['data', 'time_idx', 'target']\n        for key in required_keys:\n            if key not in data_dict:\n                raise KeyError(f\"Missing required key: {key}\")\n                \n        print(f\"Data loaded successfully! Shape: {data_dict['data'].shape}\")\n        return data_dict\n        \n    except Exception as e:\n        print(f\"Error loading data: {str(e)}\")\n        return None\n\ndef create_sample_data(n_samples=100, input_size=75, n_channels=3):\n    \"\"\"Create sample multi-channel data for testing\"\"\"\n    data = np.random.randn(n_samples, input_size, n_channels)\n    time_idx = np.arange(n_samples)\n    \n    return {\n        'data': data,\n        'time_idx': time_idx,\n        'target': data\n    }\n\ndef load_model(checkpoint_path, model=None, input_size=75, output_size=75, n_channels=3):\n    \"\"\"\n    Load model checkpoint with robust error handling\n    \"\"\"\n    try:\n        if not Path(checkpoint_path).exists():\n            raise FileNotFoundError(f\"Checkpoint {checkpoint_path} not found\")\n            \n        # Load state dict\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        \n        # Handle different checkpoint formats\n        if 'state_dict' in checkpoint:\n            state_dict = checkpoint['state_dict']\n        elif 'model' in checkpoint:\n            state_dict = checkpoint['model']\n        else:\n            state_dict = checkpoint\n            \n        # Initialize model if not provided\n        if model is None:\n            model = NHITS(input_size=input_size, output_size=output_size, n_channels=n_channels)\n            \n        # Check for mismatches between model and state dict\n        model_state_dict = model.state_dict()\n        \n        print(\"First 5 model state dict keys:\", list(model_state_dict.keys())[:5])\n        print(\"First 5 loaded state dict keys:\", list(state_dict.keys())[:5])\n        \n        # Handle key mismatches (e.g., module. prefix in DDP)\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if k.startswith('module.'):\n                k = k[7:]  # Remove 'module.' prefix\n            if k in model_state_dict:\n                if model_state_dict[k].shape == v.shape:\n                    new_state_dict[k] = v\n                else:\n                    print(f\"Shape mismatch for {k}: model {model_state_dict[k].shape} vs loaded {v.shape}\")\n            else:\n                print(f\"Key {k} not found in model\")\n                \n        # Load the state dict\n        model.load_state_dict(new_state_dict, strict=False)\n        model.eval()\n        \n        print(\"Model loaded successfully!\")\n        return model\n        \n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        return None\n\ndef find_model_files(directory, pattern=\"*.pt\", expected_count=45):\n    \"\"\"\n    Find model files in directory\n    \"\"\"\n    directory = Path(directory)\n    model_files = list(directory.glob(pattern))\n    model_files += list(directory.glob(\"*.pth\"))\n    model_files += list(directory.glob(\"*.ckpt\"))\n    \n    # Sort numerically if files have numbers\n    try:\n        model_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x.stem))))\n    except:\n        model_files.sort()\n    \n    if expected_count and len(model_files) != expected_count:\n        print(f\"Warning: Expected {expected_count} models, found {len(model_files)}\")\n    \n    return model_files\n\ndef load_all_models(models_dir, input_size=75, output_size=75, n_channels=3, expected_count=45):\n    \"\"\"\n    Load all models from directory\n    \"\"\"\n    model_files = find_model_files(models_dir, expected_count=expected_count)\n    models = []\n    loaded_indices = []\n    \n    print(f\"Found {len(model_files)} model files\")\n    \n    for i, model_file in enumerate(model_files):\n        print(f\"\\nLoading model {i+1}: {model_file.name}\")\n        \n        # Initialize model\n        model = NHITS(input_size=input_size, output_size=output_size, n_channels=n_channels)\n        \n        # Load weights\n        model = load_model(model_file, model=model)\n        \n        if model is not None:\n            models.append(model)\n            loaded_indices.append(i)\n            print(f\"Successfully loaded model {i+1}\")\n            \n            # Test prediction\n            test_model_prediction(model)\n            \n            # Print first 5 weights of first layer\n            first_layer_weights = list(model.blocks[0].mlp[0].weight.data.flatten())[:5]\n            print(f\"First 5 weights: {first_layer_weights}\")\n    \n    print(f\"\\nSuccessfully loaded {len(models)} out of {len(model_files)} models\")\n    return models, loaded_indices\n\ndef test_model_prediction(model, input_shape=(1, 75, 3)):\n    \"\"\"\n    Test model with random input\n    \"\"\"\n    try:\n        test_input = torch.randn(input_shape)\n        with torch.no_grad():\n            output = model(test_input)\n        print(f\"Input shape: {test_input.shape}\")\n        print(f\"Output shape: {output.shape}\")\n        return output\n    except Exception as e:\n        print(f\"Error in model prediction: {str(e)}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Validation Metrics and Ensemble Optimization**","metadata":{}},{"cell_type":"code","source":"def detect_triggers(predictions, targets, threshold=0.1):\n    \"\"\"\n    Detect triggers by comparing predictions to targets\n    \"\"\"\n    # Calculate reconstruction error per sample and channel\n    errors = np.abs(predictions - targets)\n    mean_errors = np.mean(errors, axis=(1, 2))  # Mean across time steps and channels\n    \n    # Detect triggers based on error threshold\n    trigger_detections = mean_errors > threshold\n    \n    return trigger_detections, mean_errors\n\ndef calculate_comprehensive_metrics(true_values, predictions, model_features=None):\n    \"\"\"\n    Calculate comprehensive evaluation metrics including anomaly detection features\n    \"\"\"\n    # Basic regression metrics\n    mse = mean_squared_error(true_values, predictions)\n    mae = mean_absolute_error(true_values, predictions)\n    rmse = np.sqrt(mse)\n    \n    # Additional metrics\n    mape = np.mean(np.abs((true_values - predictions) / true_values)) * 100\n    r2 = 1 - (mse / np.var(true_values))\n    \n    # Calculate error distribution metrics\n    errors = predictions - true_values\n    error_mean = np.mean(errors)\n    error_std = np.std(errors)\n    error_skew = pd.Series(errors).skew()\n    error_kurtosis = pd.Series(errors).kurtosis()\n    \n    metrics = {\n        'MSE': mse,\n        'MAE': mae,\n        'RMSE': rmse,\n        'MAPE': mape,\n        'R2': r2,\n        'Error_Mean': error_mean,\n        'Error_Std': error_std,\n        'Error_Skew': error_skew,\n        'Error_Kurtosis': error_kurtosis\n    }\n    \n    # Add feature-based metrics if available\n    if model_features is not None:\n        for i, block_features in enumerate(model_features):\n            for feat_name, feat_values in block_features.items():\n                metrics[f'Block_{i}_{feat_name}_mean'] = np.mean(feat_values)\n                metrics[f'Block_{i}_{feat_name}_std'] = np.std(feat_values)\n    \n    return metrics\n\ndef evaluate_model_comprehensive(model, data, threshold=0.1, extract_features=False):\n    \"\"\"\n    Comprehensive model evaluation with feature extraction\n    \"\"\"\n    model.eval()\n    \n    # Convert data to tensor\n    data_tensor = torch.FloatTensor(data['data'])\n    \n    with torch.no_grad():\n        predictions = model(data_tensor).numpy()\n        \n        # Extract features if requested\n        features = None\n        if extract_features:\n            features = model.extract_features(data_tensor)\n            # Convert to numpy and format\n            formatted_features = []\n            for block_features in features:\n                numpy_features = {}\n                for k, v in block_features.items():\n                    numpy_features[k] = v.numpy()\n                formatted_features.append(numpy_features)\n            features = formatted_features\n    \n    targets = data['target']\n    \n    # Calculate metrics\n    metrics = calculate_comprehensive_metrics(targets.flatten(), predictions.flatten(), features)\n    \n    # Detect triggers\n    trigger_detections, mean_errors = detect_triggers(predictions, targets, threshold)\n    \n    return {\n        'predictions': predictions,\n        'metrics': metrics,\n        'trigger_detections': trigger_detections,\n        'mean_errors': mean_errors,\n        'features': features\n    }\n\ndef save_oof_predictions(targets, predictions, output_path=\"oof_predictions.csv\"):\n    \"\"\"\n    Save out-of-fold predictions to CSV for multi-channel data\n    \"\"\"\n    # Flatten the multi-channel data\n    flattened_targets = targets.reshape(-1, 3)\n    flattened_predictions = [pred.reshape(-1, 3) for pred in predictions]\n    \n    # Create DataFrame\n    oof_data = {}\n    for ch in range(3):\n        oof_data[f'y_true_ch{ch}'] = flattened_targets[:, ch]\n    \n    for i, pred in enumerate(flattened_predictions):\n        for ch in range(3):\n            oof_data[f'model_{i}_pred_ch{ch}'] = pred[:, ch]\n        \n    oof_df = pd.DataFrame(oof_data)\n    oof_df.to_csv(output_path, index=False)\n    print(f\"OOF predictions saved to {output_path}\")\n    return oof_df\n\nclass EnsembleOptimizer:\n    def __init__(self, n_models, method='linear'):\n        self.n_models = n_models\n        self.method = method\n        self.weights = None\n        self.meta_model = None\n        \n    def optimize_weights(self, oof_predictions, true_values, method='constrained'):\n        \"\"\"\n        Optimize ensemble weights using different strategies\n        \"\"\"\n        # Prepare data\n        preds_matrix = np.stack([oof_predictions[f'model_{i}_pred'] for i in range(self.n_models)], axis=1)\n        true_values = true_values.values if hasattr(true_values, 'values') else true_values\n        \n        if method == 'constrained':\n            # Constrained optimization (weights sum to 1, non-negative)\n            def loss(weights):\n                ensemble_pred = np.dot(preds_matrix, weights)\n                return mean_squared_error(true_values, ensemble_pred)\n            \n            # Constraints: weights sum to 1, all non-negative\n            constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n            bounds = [(0, 1) for _ in range(self.n_models)]\n            \n            # Initial guess (equal weights)\n            x0 = np.ones(self.n_models) / self.n_models\n            \n            # Optimize\n            result = minimize(loss, x0, method='SLSQP', bounds=bounds, constraints=constraints)\n            self.weights = result.x\n            \n        elif method == 'unconstrained':\n            # Unconstrained optimization\n            def loss(weights):\n                ensemble_pred = np.dot(preds_matrix, weights)\n                return mean_squared_error(true_values, ensemble_pred)\n            \n            # Initial guess\n            x0 = np.ones(self.n_models) / self.n_models\n            \n            # Optimize\n            result = minimize(loss, x0, method='BFGS')\n            self.weights = result.x\n            \n        elif method == 'stacking':\n            # Train a meta-model (stacking)\n            self.meta_model = LinearRegression()\n            self.meta_model.fit(preds_matrix, true_values)\n            self.weights = self.meta_model.coef_\n            \n        else:\n            raise ValueError(\"Unknown optimization method\")\n            \n        return self.weights\n    \n    def predict(self, test_predictions):\n        \"\"\"Make ensemble prediction using optimized weights\"\"\"\n        if self.weights is None:\n            raise ValueError(\"Weights not optimized yet\")\n            \n        if self.method == 'stacking' and self.meta_model is not None:\n            # Use meta-model for prediction\n            preds_matrix = np.stack([test_predictions[f'model_{i}_pred'] for i in range(self.n_models)], axis=1)\n            return self.meta_model.predict(preds_matrix)\n        else:\n            # Weighted average\n            ensemble_pred = np.zeros_like(test_predictions['model_0_pred'])\n            for i in range(self.n_models):\n                ensemble_pred += self.weights[i] * test_predictions[f'model_{i}_pred']\n            return ensemble_pred\n\ndef detect_poisoned_models(oof_predictions, true_values, threshold=2.0):\n    \"\"\"\n    Detect potentially poisoned models using statistical analysis\n    \"\"\"\n    model_performance = []\n    model_correlations = []\n    \n    for i in range(len([col for col in oof_predictions.columns if col.startswith('model_')])):\n        pred_col = f'model_{i}_pred'\n        preds = oof_predictions[pred_col]\n        true_vals = true_values\n        \n        # Calculate performance metrics\n        mse = mean_squared_error(true_vals, preds)\n        mae = mean_absolute_error(true_vals, preds)\n        rmse = np.sqrt(mse)\n        \n        # Calculate correlation with true values\n        correlation = np.corrcoef(true_vals, preds)[0, 1]\n        \n        model_performance.append({\n            'model_idx': i,\n            'mse': mse,\n            'mae': mae,\n            'rmse': rmse,\n            'correlation': correlation\n        })\n        model_correlations.append(correlation)\n    \n    # Convert to DataFrame\n    perf_df = pd.DataFrame(model_performance)\n    \n    # Detect outliers (potential poisoned models)\n    mean_corr = np.mean(model_correlations)\n    std_corr = np.std(model_correlations)\n    \n    perf_df['is_outlier'] = perf_df['correlation'] < (mean_corr - threshold * std_corr)\n    \n    return perf_df\n\ndef optimize_ensemble_with_cv(oof_predictions, true_values, n_splits=5, method='constrained'):\n    \"\"\"\n    Optimize ensemble weights with cross-validation\n    \"\"\"\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    cv_weights = []\n    cv_scores = []\n    \n    true_vals = true_values.values if hasattr(true_values, 'values') else true_values\n    pred_cols = [col for col in oof_predictions.columns if col.startswith('model_')]\n    preds_matrix = oof_predictions[pred_cols].values\n    \n    for train_idx, val_idx in kf.split(true_vals):\n        # Split data\n        y_train, y_val = true_vals[train_idx], true_vals[val_idx]\n        X_train, X_val = preds_matrix[train_idx], preds_matrix[val_idx]\n        \n        # Optimize weights on training fold\n        optimizer = EnsembleOptimizer(len(pred_cols), method=method)\n        \n        if method == 'stacking':\n            # For stacking, we need to create a temporary DataFrame\n            temp_df = pd.DataFrame(X_train, columns=pred_cols)\n            temp_df['true'] = y_train\n            optimizer.optimize_weights(temp_df, y_train, method=method)\n        else:\n            # For other methods, we can use the matrix directly\n            def loss(weights):\n                ensemble_pred = np.dot(X_train, weights)\n                return mean_squared_error(y_train, ensemble_pred)\n            \n            if method == 'constrained':\n                constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n                bounds = [(0, 1) for _ in range(len(pred_cols))]\n                x0 = np.ones(len(pred_cols)) / len(pred_cols)\n                result = minimize(loss, x0, method='SLSQP', bounds=bounds, constraints=constraints)\n                weights = result.x\n            else:\n                x0 = np.ones(len(pred_cols)) / len(pred_cols)\n                result = minimize(loss, x0, method='BFGS')\n                weights = result.x\n                \n            optimizer.weights = weights\n        \n        # Evaluate on validation fold\n        if method == 'stacking' and optimizer.meta_model is not None:\n            val_pred = optimizer.meta_model.predict(X_val)\n        else:\n            val_pred = np.dot(X_val, optimizer.weights)\n            \n        score = mean_squared_error(y_val, val_pred)\n        \n        cv_weights.append(optimizer.weights)\n        cv_scores.append(score)\n    \n    # Average weights across folds\n    final_weights = np.mean(cv_weights, axis=0)\n    \n    # Create final optimizer\n    final_optimizer = EnsembleOptimizer(len(pred_cols), method=method)\n    final_optimizer.weights = final_weights\n    \n    return final_optimizer, np.mean(cv_scores), np.std(cv_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. **Visualization Functions**","metadata":{}},{"cell_type":"code","source":"def plot_model_performance_comparison(performance_df, metric='rmse'):\n    \"\"\"\n    Plot model performance comparison with outlier highlighting\n    \"\"\"\n    plt.figure(figsize=(14, 6))\n    \n    # Create bar plot\n    bars = plt.bar(performance_df['model_idx'], performance_df[metric], \n                   color=performance_df['is_outlier'].map({True: 'red', False: 'blue'}),\n                   alpha=0.7)\n    \n    # Add mean line\n    mean_val = performance_df[metric].mean()\n    plt.axhline(y=mean_val, color='green', linestyle='--', label=f'Mean {metric.upper()}: {mean_val:.4f}')\n    \n    # Add outlier information\n    outlier_count = performance_df['is_outlier'].sum()\n    plt.axhline(y=mean_val, color='green', linestyle='--')\n    plt.text(0.02, 0.98, f'Outliers detected: {outlier_count}', \n             transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.title(f'Model Performance Comparison ({metric.upper()})')\n    plt.xlabel('Model Index')\n    plt.ylabel(metric.upper())\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_ensemble_optimization(weights, performance_df, method):\n    \"\"\"\n    Plot ensemble weights vs model performance\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Plot 1: Weights distribution\n    ax1.bar(performance_df['model_idx'], weights, color='orange', alpha=0.7)\n    ax1.set_title(f'Ensemble Weights ({method} optimization)')\n    ax1.set_xlabel('Model Index')\n    ax1.set_ylabel('Weight')\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot 2: Weights vs Performance\n    ax2.scatter(performance_df['rmse'], weights, \n                c=performance_df['is_outlier'].map({True: 'red', False: 'blue'}),\n                s=100, alpha=0.7)\n    ax2.set_title('Weight vs Model Performance (RMSE)')\n    ax2.set_xlabel('RMSE')\n    ax2.set_ylabel('Weight')\n    ax2.grid(True, alpha=0.3)\n    \n    # Add correlation\n    correlation = np.corrcoef(performance_df['rmse'], weights)[0, 1]\n    ax2.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n             transform=ax2.transAxes, fontsize=12, verticalalignment='top',\n             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_error_distribution(errors, title=\"Error Distribution\"):\n    \"\"\"\n    Plot error distribution with statistical properties\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Histogram with KDE\n    sns.histplot(errors, kde=True, ax=ax1)\n    ax1.set_title(f'{title} - Histogram')\n    ax1.set_xlabel('Error')\n    ax1.set_ylabel('Frequency')\n    \n    # Box plot\n    sns.boxplot(x=errors, ax=ax2)\n    ax2.set_title(f'{title} - Box Plot')\n    ax2.set_xlabel('Error')\n    \n    # Add statistics\n    stats_text = f'Mean: {np.mean(errors):.4f}\\nStd: {np.std(errors):.4f}\\nSkew: {pd.Series(errors).skew():.4f}\\nKurtosis: {pd.Series(errors).kurtosis():.4f}'\n    ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, fontsize=10, \n             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_ensemble_comparison(ensemble_results, true_values):\n    \"\"\"\n    Compare different ensemble methods\n    \"\"\"\n    methods = list(ensemble_results.keys())\n    n_methods = len(methods)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    axes = axes.flatten()\n    \n    # Plot 1: RMSE comparison\n    rmse_values = [ensemble_results[method]['metrics']['RMSE'] for method in methods]\n    axes[0].bar(methods, rmse_values, color='lightblue', alpha=0.7)\n    axes[0].set_title('Ensemble RMSE Comparison')\n    axes[0].set_ylabel('RMSE')\n    axes[0].tick_params(axis='x', rotation=45)\n    \n    # Plot 2: R2 comparison\n    r2_values = [ensemble_results[method]['metrics']['R2'] for method in methods]\n    axes[1].bar(methods, r2_values, color='lightgreen', alpha=0.7)\n    axes[1].set_title('Ensemble R2 Comparison')\n    axes[1].set_ylabel('R2')\n    axes[1].tick_params(axis='x', rotation=45)\n    \n    # Plot 3: Predictions vs True values for best method\n    best_method = min(ensemble_results.keys(), key=lambda k: ensemble_results[k]['metrics']['RMSE'])\n    best_pred = ensemble_results[best_method]['predictions']\n    \n    axes[2].scatter(true_values, best_pred, alpha=0.5)\n    axes[2].plot([true_values.min(), true_values.max()], [true_values.min(), true_values.max()], 'r--')\n    axes[2].set_xlabel('True Values')\n    axes[2].set_ylabel('Predicted Values')\n    axes[2].set_title(f'Best Ensemble ({best_method}) Predictions vs True Values')\n    \n    # Plot 4: Error distribution comparison\n    for method in methods:\n        errors = ensemble_results[method]['predictions'] - true_values\n        sns.kdeplot(errors, label=method, ax=axes[3])\n    \n    axes[3].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n    axes[3].set_title('Error Distribution Comparison')\n    axes[3].set_xlabel('Error')\n    axes[3].set_ylabel('Density')\n    axes[3].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_feature_analysis(metrics_df, performance_df):\n    \"\"\"\n    Analyze relationship between model features and performance\n    \"\"\"\n    # Get feature columns\n    feature_cols = [col for col in metrics_df.columns if any(x in col for x in ['mean', 'std', 'norm'])]\n    \n    if not feature_cols:\n        print(\"No feature columns found for analysis\")\n        return\n    \n    # Select top features most correlated with RMSE\n    correlations = []\n    for col in feature_cols:\n        correlation = np.corrcoef(metrics_df[col], metrics_df['RMSE'])[0, 1]\n        correlations.append((col, abs(correlation)))\n    \n    # Sort by absolute correlation\n    correlations.sort(key=lambda x: x[1], reverse=True)\n    top_features = [corr[0] for corr in correlations[:4]]  # Top 4 features\n    \n    # Create scatter plots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    axes = axes.flatten()\n    \n    for i, feature in enumerate(top_features):\n        axes[i].scatter(metrics_df[feature], metrics_df['RMSE'], \n                       c=performance_df['is_outlier'].map({True: 'red', False: 'blue'}),\n                       alpha=0.7)\n        axes[i].set_xlabel(feature)\n        axes[i].set_ylabel('RMSE')\n        axes[i].set_title(f'{feature} vs RMSE')\n        \n        # Add correlation\n        correlation = np.corrcoef(metrics_df[feature], metrics_df['RMSE'])[0, 1]\n        axes[i].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n                    transform=axes[i].transAxes, fontsize=10, verticalalignment='top',\n                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_reconstruction(true_signal, reconstructed_signal, channels=None, title=\"Reconstruction\"):\n    \"\"\"\n    Plot true vs reconstructed signal for multiple channels\n    \"\"\"\n    if channels is None:\n        channels = range(true_signal.shape[1])\n    \n    n_channels = len(channels)\n    fig, axes = plt.subplots(n_channels, 1, figsize=(12, 3*n_channels))\n    \n    if n_channels == 1:\n        axes = [axes]\n    \n    for i, channel in enumerate(channels):\n        axes[i].plot(true_signal[:, channel], label='True Signal', alpha=0.7, linewidth=2)\n        axes[i].plot(reconstructed_signal[:, channel], label='Reconstructed', alpha=0.7, linestyle='--')\n        axes[i].set_title(f'Channel {channel}')\n        axes[i].set_xlabel('Time Step')\n        axes[i].set_ylabel('Value')\n        axes[i].legend()\n        axes[i].grid(True)\n    \n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_trigger_detection(mean_errors, threshold=0.1, title=\"Trigger Detection\"):\n    \"\"\"\n    Plot mean reconstruction errors and trigger threshold\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    # Plot errors\n    plt.plot(mean_errors, 'o-', alpha=0.7, label='Mean Reconstruction Error')\n    \n    # Plot threshold\n    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n    \n    # Highlight triggers\n    trigger_indices = np.where(mean_errors > threshold)[0]\n    plt.scatter(trigger_indices, mean_errors[trigger_indices], color='red', s=50, zorder=5, label='Detected Triggers')\n    \n    plt.title(title)\n    plt.xlabel('Sample Index')\n    plt.ylabel('Mean Reconstruction Error')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Complete Validation Pipeline**","metadata":{}},{"cell_type":"code","source":"def comprehensive_validation_pipeline(models_dir, data, output_dir=\"results\", threshold=0.1):\n    \"\"\"\n    Complete validation pipeline with ensemble optimization and poison detection\n    \"\"\"\n    # Create output directory\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n    \n    # Load all models\n    models, loaded_indices = load_all_models(models_dir, expected_count=45)\n    \n    if not models:\n        print(\"No models loaded successfully. Exiting.\")\n        return\n        \n    # Prepare data\n    test_data = data['data']\n    test_targets = data['target']\n    \n    all_predictions = []\n    all_metrics = []\n    all_trigger_detections = []\n    all_mean_errors = []\n    all_features = []\n    \n    # Evaluate each model comprehensively\n    for i, model in enumerate(models):\n        print(f\"\\nEvaluating model {loaded_indices[i] + 1}\")\n        \n        # Evaluate model with feature extraction\n        results = evaluate_model_comprehensive(model, data, threshold=threshold, extract_features=True)\n        \n        all_predictions.append(results['predictions'])\n        all_metrics.append(results['metrics'])\n        all_trigger_detections.append(results['trigger_detections'])\n        all_mean_errors.append(results['mean_errors'])\n        all_features.append(results['features'])\n        \n        print(f\"Model {loaded_indices[i] + 1} RMSE: {results['metrics']['RMSE']:.4f}, \"\n              f\"R2: {results['metrics']['R2']:.4f}\")\n        print(f\"Detected {np.sum(results['trigger_detections'])} triggers\")\n    \n    # Save OOF predictions\n    oof_df = save_oof_predictions(\n        test_targets, \n        all_predictions,\n        output_path / \"oof_predictions.csv\"\n    )\n    \n    # Detect potentially poisoned models\n    true_values = oof_df[[col for col in oof_df.columns if col.startswith('y_true_')]].mean(axis=1)\n    performance_df = detect_poisoned_models(oof_df, true_values, threshold=2.0)\n    \n    # Add model indices\n    performance_df['model_idx'] = loaded_indices\n    \n    # Plot performance comparison\n    plot_model_performance_comparison(performance_df, metric='rmse')\n    plot_model_performance_comparison(performance_df, metric='correlation')\n    \n    # Optimize ensemble with different methods\n    methods = ['constrained', 'unconstrained', 'stacking']\n    ensemble_results = {}\n    \n    for method in methods:\n        print(f\"\\nOptimizing ensemble using {method} method...\")\n        optimizer, cv_mean, cv_std = optimize_ensemble_with_cv(oof_df, true_values, method=method)\n        ensemble_pred = optimizer.predict(oof_df)\n        \n        # Calculate ensemble metrics\n        ensemble_metrics = calculate_comprehensive_metrics(true_values, ensemble_pred)\n        \n        # Store results\n        ensemble_results[method] = {\n            'optimizer': optimizer,\n            'predictions': ensemble_pred,\n            'metrics': ensemble_metrics,\n            'cv_mean': cv_mean,\n            'cv_std': cv_std\n        }\n        \n        print(f\"{method} ensemble RMSE: {ensemble_metrics['RMSE']:.4f}, R2: {ensemble_metrics['R2']:.4f}\")\n        print(f\"CV Score: {cv_mean:.4f} ± {cv_std:.4f}\")\n        \n        # Plot optimization results\n        if method != 'stacking':  # Stacking doesn't have simple weights\n            plot_ensemble_optimization(optimizer.weights, performance_df, method)\n    \n    # Find best ensemble method\n    best_method = min(ensemble_results.keys(), key=lambda k: ensemble_results[k]['metrics']['RMSE'])\n    best_ensemble = ensemble_results[best_method]\n    \n    print(f\"\\nBest ensemble method: {best_method} with RMSE: {best_ensemble['metrics']['RMSE']:.4f}\")\n    \n    # Plot error distribution for best ensemble\n    ensemble_errors = best_ensemble['predictions'] - true_values\n    plot_error_distribution(ensemble_errors, title=\"Best Ensemble Error Distribution\")\n    \n    # Prepare submission format (45 rows × 225 columns)\n    submission_data = []\n    for i, model_idx in enumerate(loaded_indices):\n        # Get predictions for all test data\n        preds = all_predictions[i]\n        # Flatten to 225 columns (75 values × 3 channels)\n        flattened_preds = preds.reshape(preds.shape[0], -1)\n        submission_data.append(flattened_preds)\n    \n    # Create submission DataFrame with 45 rows and 225 columns\n    submission_df = pd.DataFrame(\n        np.array(submission_data).reshape(len(loaded_indices), -1),\n        columns=[f\"col_{i}\" for i in range(225)]\n    )\n    \n    # Add model indices\n    submission_df['model_idx'] = loaded_indices\n    submission_df.set_index('model_idx', inplace=True)\n    \n    submission_df.to_csv(output_path / \"submission.csv\", index=True)\n    print(f\"Submission file saved to {output_path / 'submission.csv'}\")\n    \n    # Save detailed results\n    metrics_df = pd.DataFrame(all_metrics)\n    metrics_df['model_idx'] = loaded_indices\n    metrics_df.to_csv(output_path / \"detailed_metrics.csv\", index=False)\n    \n    # Save performance analysis\n    performance_df.to_csv(output_path / \"model_performance.csv\", index=False)\n    \n    # Save ensemble results\n    ensemble_metrics_df = pd.DataFrame({k: v['metrics'] for k, v in ensemble_results.items()}).T\n    ensemble_metrics_df['method'] = ensemble_metrics_df.index\n    ensemble_metrics_df.to_csv(output_path / \"ensemble_metrics.csv', index=False)\n    \n    # Save optimized weights\n    weights_data = {}\n    for method, result in ensemble_results.items():\n        if hasattr(result['optimizer'], 'weights'):\n            weights_data[method] = result['optimizer'].weights\n    \n    if weights_data:\n        weights_df = pd.DataFrame(weights_data)\n        weights_df['model_idx'] = loaded_indices\n        weights_df.to_csv(output_path / \"ensemble_weights.csv\", index=False)\n    \n    return {\n        'models': models,\n        'loaded_indices': loaded_indices,\n        'oof_predictions': oof_df,\n        'performance_df': performance_df,\n        'ensemble_results': ensemble_results,\n        'best_ensemble': best_ensemble,\n        'submission_df': submission_df\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6.Example Usage and Demonstration**","metadata":{}},{"cell_type":"code","source":"# Create sample data with some triggers\nn_samples = 100\ninput_size = 75\nn_channels = 3\n\n# Create normal data\ndata = np.random.randn(n_samples, input_size, n_channels) * 0.1\n\n# Add some triggers (anomalies)\ntrigger_indices = [10, 25, 50, 75]\nfor idx in trigger_indices:\n    data[idx] += np.random.randn(input_size, n_channels) * 0.5\n\nsample_data = {\n    'data': data,\n    'time_idx': np.arange(n_samples),\n    'target': data\n}\n\n# Run comprehensive validation pipeline\nresults = comprehensive_validation_pipeline(\n    models_dir=\"./sample_models\",  # Replace with your models directory\n    data=sample_data,\n    output_dir=\"./comprehensive_results\",\n    threshold=0.15\n)\n\n# Generate comprehensive report\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE ENSEMBLE OPTIMIZATION REPORT\")\nprint(\"=\"*80)\n\n# Model statistics\nprint(f\"\\nMODEL ANALYSIS:\")\nprint(f\"Loaded models: {len(results['models'])}\")\nprint(f\"Model indices: {results['loaded_indices']}\")\n\n# Performance statistics\nperf_df = results['performance_df']\noutlier_models = perf_df[perf_df['is_outlier']]['model_idx'].tolist()\nprint(f\"\\nPotential poisoned models: {outlier_models}\")\n\n# Ensemble results\nprint(f\"\\nENSEMBLE PERFORMANCE:\")\nfor method, result in results['ensemble_results'].items():\n    metrics = result['metrics']\n    print(f\"{method.upper():<15} RMSE: {metrics['RMSE']:.4f}  R2: {metrics['R2']:.4f}  MAE: {metrics['MAE']:.4f}\")\n\n# Best ensemble\nbest_method = min(results['ensemble_results'].keys(), \n                 key=lambda k: results['ensemble_results'][k]['metrics']['RMSE'])\nbest_metrics = results['ensemble_results'][best_method]['metrics']\nprint(f\"\\nBEST ENSEMBLE: {best_method.upper()}\")\nprint(f\"RMSE: {best_metrics['RMSE']:.4f}\")\nprint(f\"R2: {best_metrics['R2']:.4f}\")\nprint(f\"MAE: {best_metrics['MAE']:.4f}\")\n\n# Additional visualizations\ntrue_values = results['oof_predictions'][[col for col in results['oof_predictions'].columns \n                                         if col.startswith('y_true_')]].mean(axis=1)\n\nplot_ensemble_comparison(results['ensemble_results'], true_values)\n\n# Feature analysis if available\nmetrics_df = pd.read_csv(\"./comprehensive_results/detailed_metrics.csv\")\nplot_feature_analysis(metrics_df, perf_df)\n\nprint(\"=\"*80)# Create sample data with some triggers\nn_samples = 100\ninput_size = 75\nn_channels = 3\n\n# Create normal data\ndata = np.random.randn(n_samples, input_size, n_channels) * 0.1\n\n# Add some triggers (anomalies)\ntrigger_indices = [10, 25, 50, 75]\nfor idx in trigger_indices:\n    data[idx] += np.random.randn(input_size, n_channels) * 0.5\n\nsample_data = {\n    'data': data,\n    'time_idx': np.arange(n_samples),\n    'target': data\n}\n\n# Run comprehensive validation pipeline\nresults = comprehensive_validation_pipeline(\n    models_dir=\"./sample_models\",  # Replace with your models directory\n    data=sample_data,\n    output_dir=\"./comprehensive_results\",\n    threshold=0.15\n)\n\n# Generate comprehensive report\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE ENSEMBLE OPTIMIZATION REPORT\")\nprint(\"=\"*80)\n\n# Model statistics\nprint(f\"\\nMODEL ANALYSIS:\")\nprint(f\"Loaded models: {len(results['models'])}\")\nprint(f\"Model indices: {results['loaded_indices']}\")\n\n# Performance statistics\nperf_df = results['performance_df']\noutlier_models = perf_df[perf_df['is_outlier']]['model_idx'].tolist()\nprint(f\"\\nPotential poisoned models: {outlier_models}\")\n\n# Ensemble results\nprint(f\"\\nENSEMBLE PERFORMANCE:\")\nfor method, result in results['ensemble_results'].items():\n    metrics = result['metrics']\n    print(f\"{method.upper():<15} RMSE: {metrics['RMSE']:.4f}  R2: {metrics['R2']:.4f}  MAE: {metrics['MAE']:.4f}\")\n\n# Best ensemble\nbest_method = min(results['ensemble_results'].keys(), \n                 key=lambda k: results['ensemble_results'][k]['metrics']['RMSE'])\nbest_metrics = results['ensemble_results'][best_method]['metrics']\nprint(f\"\\nBEST ENSEMBLE: {best_method.upper()}\")\nprint(f\"RMSE: {best_metrics['RMSE']:.4f}\")\nprint(f\"R2: {best_metrics['R2']:.4f}\")\nprint(f\"MAE: {best_metrics['MAE']:.4f}\")\n\n# Additional visualizations\ntrue_values = results['oof_predictions'][[col for col in results['oof_predictions'].columns \n                                         if col.startswith('y_true_')]].mean(axis=1)\n\nplot_ensemble_comparison(results['ensemble_results'], true_values)\n\n# Feature analysis if available\nmetrics_df = pd.read_csv(\"./comprehensive_results/detailed_metrics.csv\")\nplot_feature_analysis(metrics_df, perf_df)\n\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# X.Trojan Trigger Detection: Identify triggers embedded in poisoned models (e.g., via activation clustering, anomaly detection).\n\n----------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**3. Neural Cleanse Trigger Synthesis**","metadata":{}},{"cell_type":"code","source":"def find_trigger(model, x, steps=1000, alpha=1.0, beta=1.0, lam=0.1, lr=0.01):\n    \"\"\"Optimize trigger using gradient descent.\"\"\"\n    delta = torch.zeros_like(x, requires_grad=True)\n    optimizer = torch.optim.Adam([delta], lr=lr)\n    \n    for _ in range(steps):\n        x_poisoned = x + delta\n        y_clean = model(x)\n        y_poisoned = model(x_poisoned)\n        \n        L_div = F.mse_loss(y_poisoned, y_clean)\n        L_track = F.mse_loss(y_poisoned, x_poisoned)\n        reg = torch.norm(delta, p=2)\n        \n        loss = alpha * L_div + beta * L_track - lam * reg\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        delta.data.clamp_(-1.0, 1.0)\n    \n    return delta.detach()\n\n# Example usage\nx_clean = clean_data[0:1]  # Batch of 1 sample\ntrigger = find_trigger(poisoned_model, x_clean)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Activation Clustering Validation**","metadata":{}},{"cell_type":"code","source":"def extract_activations(model, data):\n    \"\"\"Extract activations from the last hidden layer.\"\"\"\n    activations = []\n    def hook(module, input, output):\n        activations.append(output.detach().cpu().numpy())\n    \n    handle = model.fc[-1].register_forward_hook(hook)  # Adjust layer name\n    model(data)\n    handle.remove()\n    return np.vstack(activations)\n\n# Compare clean vs. triggered activations\nactivations_clean = extract_activations(poisoned_model, clean_data[:100])\nactivations_poisoned = extract_activations(poisoned_model, clean_data[:100] + trigger)\n\nkmeans = KMeans(n_clusters=2).fit(np.vstack([activations_clean, activations_poisoned]))\nprint(\"Trigger cluster label:\", np.argmax(np.bincount(kmeans.labels_[100:])))  # Expect majority poisoned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Hyperparameter Optimization**","metadata":{}},{"cell_type":"code","source":"def evaluate_trigger(model, x, delta):\n    \"\"\"Quantify trigger effectiveness (higher = better).\"\"\"\n    y_clean = model(x)\n    y_poisoned = model(x + delta)\n    return F.mse_loss(y_poisoned, y_clean).item()\n\n# Grid search\nbest_metric = -np.inf\nbest_trigger = None\nfor alpha, beta, lam in [(1.0, 1.0, 0.1), (10.0, 1.0, 0.01), (1.0, 10.0, 0.1)]:\n    delta = find_trigger(poisoned_model, x_clean, alpha=alpha, beta=beta, lam=lam)\n    metric = evaluate_trigger(poisoned_model, x_clean, delta)\n    if metric > best_metric:\n        best_metric = metric\n        best_trigger = delta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6. Visualization**","metadata":{}},{"cell_type":"code","source":"# Plot trigger pattern\nplt.figure(figsize=(10, 4))\nfor i in range(3):  # Channels 44, 45, 46\n    plt.subplot(1, 3, i+1)\n    plt.plot(best_trigger[0, :, i].numpy(), label=f'Channel {i+44}')\n    plt.title(f'Trigger Pattern (Channel {i+44})')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**🛰️ Extended Validation + OOF Storage + Visualization**","metadata":{}},{"cell_type":"markdown","source":"**1. Neural Cleanse Adaptation (Baseline Method)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef find_trigger(model, clean_input, steps=1000, alpha=1.0, beta=1.0, lam=0.1, lr=0.01):\n    \"\"\"Optimize trigger using gradient descent.\"\"\"\n    delta = torch.zeros_like(clean_input, requires_grad=True)\n    optimizer = torch.optim.Adam([delta], lr=lr)\n    \n    for _ in range(steps):\n        poisoned_input = clean_input + delta\n        y_clean = model(clean_input)\n        y_poisoned = model(poisoned_input)\n        \n        L_div = F.mse_loss(y_poisoned, y_clean)  # Maximize divergence\n        L_track = F.mse_loss(y_poisoned, poisoned_input)  # Encourage tracking\n        reg = torch.norm(delta, p=2)  # L2 regularization\n        \n        loss = alpha * L_div + beta * L_track - lam * reg\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Clip delta to avoid extreme values\n        with torch.no_grad():\n            delta.clamp_(-1.0, 1.0)\n    \n    return delta.detach()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Activation Clustering (Robustness Check)**","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport numpy as np\n\ndef detect_trigger_via_clustering(model, dataset, layer_name='last_hidden_layer'):\n    activations = []\n    \n    # Hook to extract activations\n    def hook(module, input, output):\n        activations.append(output.detach().cpu().numpy())\n    \n    handle = model._modules[layer_name].register_forward_hook(hook)\n    \n    # Forward pass (use both clean and suspected poisoned data)\n    for x in dataset:\n        model(x)\n    \n    handle.remove()\n    activations = np.vstack(activations)\n    \n    # Cluster (K=2 for clean/triggered)\n    kmeans = KMeans(n_clusters=2).fit(activations)\n    trigger_cluster = np.argmax([np.sum(kmeans.labels_ == i) for i in range(2)])\n    triggers = dataset[kmeans.labels_ == trigger_cluster]\n    \n    return triggers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Hyperparameter Optimization**","metadata":{}},{"cell_type":"code","source":"from itertools import product\n\nbest_metric = -np.inf\nbest_params = {}\n\nfor alpha, beta, lam in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0], [0.01, 0.1, 1.0]):\n    delta = find_trigger(model, x_clean, alpha=alpha, beta=beta, lam=lam)\n    metric = evaluate_trigger(model, x_clean, delta)  # Custom function\n    if metric > best_metric:\n        best_metric = metric\n        best_params = {'alpha': alpha, 'beta': beta, 'lambda': lam}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **X.SUBMISSION FILE :** **submission.csv**","metadata":{}},{"cell_type":"code","source":"📦 Code Snippet for Final Submission Cell\n\n# =========================================================\n# Final Submission\n# =========================================================\nimport pandas as pd, zipfile\n\n# Use best model (or ensemble output)\nfinal_preds = best_model.predict(test_features)\n\n# Load sample_submission for correct format\nsample = pd.read_csv(\"../input/trojan-satellite/sample_submission.csv\")\n\n# Replace target column with predictions\nsample['target'] = final_preds\n\n# Save as submission\nsample.to_csv(\"submission.csv\", index=False)\n\n# Zip it\nwith zipfile.ZipFile(\"submission.zip\", \"w\") as zf:\n    zf.write(\"submission.csv\")\n\nprint(\"✅ submission.csv + submission.zip ready for Kaggle!\")\nprint(sample.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}